# -*- coding: utf-8 -*-
from __future__ import division

## references ##
#job machine learning bugra
#https://www.cbinsights.com/jobs
#ipython
#http://www.gibiansky.com/blog/ipython/ipython-kernels/
#compresssibility
#https://news.ycombinator.com/item?id=5998675
#bloom filter python many:many
#http://zacharyvoase.com/2012/08/31/m2mbloom/
#factorization diagram
#http://mathlesstraveled.com/factorization/
#waldboost http://www.lucaamore.com/?p=638%20%20%D0%A0%D0%B5%D0%B9%D1%82%D0%B8%D0%BD%D0%B3
#http://mhr3.blogspot.com/2012/03/face-detection-with-opencl.html
#job computer vision
#http://cs.stackexchange.com/questions/16697/job-sites-for-applied-interdisciplinary-mathematics-related-to-computer-science?rq=1
#beamer how to design a theme
#http://www.drbunsen.org/designing-a-beamer-template-theme/
#https://github.com/drbunsen/drbunsen-beamer
#random numbers
#http://www.analyticbridge.com/forum/topics/challenge-of-the-week-random-numbers
#cloud sagemath
#https://cloud.sagemath.com/
# d3.js radar plot (weather)
#http://yr.hveem.no/
#data science as career
#http://jakevdp.github.io/blog/2014/08/22/hacking-academia/
#causality
#http://blog.echen.me/2014/08/15/propensity-modeling-causal-inference-and-discovering-drivers-of-growth/

#ipython notebook style
#ipython notebook style
#plots reproducible
#http://damon-is-a-geek.com/publication-ready-the-first-time-beautiful-reproducible-plots-with-matplotlib.html
#from IPython.core.display import HTML
#css_file = './custom.css'
#HTML(open(css_file, "r").read())
#http://lorenabarba.com/blog/announcing-aeropython/
#http://nbviewer.ipython.org/github/ketch/HyperPython/blob/master/Lesson_03_High-resolution_methods.ipynb
#http://www.davidketcheson.info/2014/05/28/hyperpython.html
#http://slendermeans.org/better-typography-for-ipython-notebooks.html
#http://blog.henryhhammond.com/theming-ipython/
#http://nbviewer.ipython.org/github/hugadams/imgproc_supplemental/blob/master/Notebooks/NBCONFIG.ipynb
#ipython notebook stat analysis
#https://github.com/fonnesbeck/statistical-analysis-python-tutorial
#data analysis 
#http://www.bayesimpact.org/blog/walking-the-beat.html
#flame graph
#job surgeryacademy glass for surgery: https://angel.co/surgery-academy/jobs?utm_campaign=talent&utm_content=promote_jobs_box&utm_medium=al_tweet&utm_source=twitter&utm_term=surgery-academy
#datomic http://blog.datomic.com/2014/08/stuff-happens-fixing-bad-data-in-datomic.html
#which distribution do I have?
#http://www.vosesoftware.com/content/ebook.pdf
#http://www.johndcook.com/blog/2010/08/11/what-distribution-does-my-data-have/
#bayesian http://www.johndcook.com/blog/tag/bayesian/
#get data sources
#http://www.quandl.com/
#correlation
#http://www.chrisstucchio.com/blog/2014/existence_does_not_imply_correlation.html
#p-vals research paper
#https://peerj.com/preprints/447v3.pdf
#ipython
#ipython notebook --pylab=inline
#design
#https://news.ycombinator.com/item?id=8182084
#random numbers 
#http://www.analyticbridge.com/forum/topics/challenge-of-the-week-random-numbers
#datascience refs
#http://www.datasciencecentral.com/profiles/blogs/data-science-cheat-sheet
#data viz
#http://www.reddit.com/r/dataisbeautiful/comments/2djft8/best_of_dataisbeautiful_january_july_2014/
#hockey stick graph climate timeseries
#https://en.wikipedia.org/wiki/Hockey_stick_graph
#traffic control design
#http://ops.fhwa.dot.gov/publications/fhwahop08024/chapter4.htm
#sample size
#http://jvns.ca/blog/2014/07/11/fun-with-stats-how-big-of-a-sample-size-do-i-need/
#phd blog
#http://www-ui.is.s.u-tokyo.ac.jp/~ume/
#fisher-yates randomization
#http://spin.atomicobject.com/2014/08/11/fisher-yates-shuffle-randomization-algorithm/
#fuzzy
#http://lcamtuf.blogspot.com/2014/08/binary-fuzzing-strategies-what-works.html
#js functional programming
#https://stevekrouse.github.io/hs.js/
#chicago dataanalytics firm
# work@datascopeanalytics.com
#django jobs
#https://djangogigs.com/gigs/country/united-states/
#reservoir sampling
#http://slantedwindows.com/reservoir-sampling-made-visual/
#russian medical decision tree
#http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/
#jobs
#http://www.datatau.com/item?id=3398
#random forest phd thesis
#https://github.com/glouppe/phd-thesis
#python
#http://anandology.com/python-practice-book/index.html
#network anomoly topological
#http://unsupervisedlearning.wordpress.com/2014/08/04/topological-anomaly-detection/
#wordvector
#http://www.socher.org/index.php/Main/GloveGlobalVectorsForWordRepresentation
#optimal streaming histogram
#http://blog.amplitude.com/2014/08/06/optimal-streaming-histograms/
#kaggle ensemble learning , rank averaging, stacking
#http://mlwave.com/reflecting-back-on-one-year-of-kaggle-contests/
#backprop
#http://numericinsight.blogspot.com/2014/07/a-gentle-introduction-to-backpropagation.html
#health testing
#http://news.harvard.edu/gazette/story/2014/08/cheap-and-compact-medical-testing/
#healthIT
#http://techcrunch.com/2014/08/05/backed-by-yc-and-rock-health-aptible-handles-the-hard-parts-of-hipaa-compliance/
#datastructure
#http://obviam.net/index.php/game-ai-an-introduction-to-behavior-trees/
#bayes 
#http://blog.henryhhammond.com/kcbo-a-bayesian-data-analysis-toolkit/
#healthIT app
#http://ychacks.challengepost.com/submissions/25781-athelas
#bayes ab testing
#http://conversion-rate.appspot.com/
#disease graph relation
#http://www.diseasegraph.com/
#game nash sim
#https://github.com/pdtournament/pdtournament
#healthIT app
#http://rxv2.com/#Quit-Smoking
#academic misleading
#http://sss.sagepub.com/content/44/4/638.long
#speedlimit 85% no variability
#http://priceonomics.com/is-every-speed-limit-too-low/
#audio extract
#http://newsoffice.mit.edu/2014/algorithm-recovers-speech-from-vibrations-0804
#epidemic
#http://en.wikipedia.org/wiki/Friendship_paradox
#type language
#http://spacemanaki.com/blog/2014/08/04/Just-LOOK-at-the-humongous-type/
#control
#http://web.mit.edu/remy/
#sql
#http://wozniak.ca/what-orms-have-taught-me-just-learn-sql
#diagonlization
#http://rjlipton.wordpress.com/2014/08/03/diagonalization-without-sets/
#markov visualization
#http://setosa.io/blog/2014/07/26/markov-chains/index.html
#collatz sequence clojure
#http://www.petrounias.org/articles/2014/08/03/collatz-sequence-generation-performance-profiling-in-clojure/
#lda notebook
#http://sebastianraschka.com/Articles/2014_python_lda.html
#ebola spread
#http://grantbrown.github.io/libspatialSEIR/doc/tutorials/Ebola2014/Ebola2014.html
#pvals change
#http://www.newyorker.com/magazine/2010/12/13/the-truth-wears-off?currentPage=all
'''
Palmer’s most convincing evidence relies on a statistical tool known as a funnel graph. When a large number of studies have been done on a single subject, the data should follow a pattern: studies with a large sample size should all cluster around a common value—the true result—whereas those with a smaller sample size should exhibit a random scattering, since they’re subject to greater sampling error. This pattern gives the graph its name, since the distribution resembles a funnel.

This suggests that the decline effect is actually a decline of illusion. While Karl Popper imagined falsification occurring with a single, definitive experiment—Galileo refuted Aristotelian mechanics in an afternoon—the process turns out to be much messier than that. Many scientific theories continue to be considered true even after failing numerous experimental tests. Verbal overshadowing might exhibit the decline effect, but it remains extensively relied upon within the field. The same holds for any number of phenomena, from the disappearing benefits of second-generation antipsychotics to the weak coupling ratio exhibited by decaying neutrons, which appears to have fallen by more than ten standard deviations between 1969 and 2001. Even the law of gravity hasn’t always been perfect at predicting real-world phenomena. (In one test, physicists measuring gravity by means of deep boreholes in the Nevada desert found a two-and-a-half-per-cent discrepancy between the theoretical predictions and the actual data.) Despite these findings, second-generation antipsychotics are still widely prescribed, and our model of the neutron hasn’t changed. The law of gravity remains the same. 



'''

#text processing
#http://matt.might.net/articles/sculpting-text/
#industry
#http://techcrunch.com/2014/07/22/boardrounds-dorm-room-fund/
#jobs
#http://priceonomics.com/join-the-priceonomics-writing-team/
#outliers frequency trail
#http://www.brendangregg.com/frequencytrails.html
#what cause outlier? distribution change over time?
#modes http://www.brendangregg.com/FrequencyTrails/modes.html
#motivation
#http://genius.cat-v.org/richard-feynman/writtings/letters/problems
#effect size
#http://stats.stackexchange.com/questions/90668/bayesian-analysis-of-contingency-tables-how-to-describe-effect-size?rq=1
#sequential notebook
#https://github.com/rasbt/python_reference

#id private
#https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange#Description
#https://freedom-to-tinker.com/blog/randomwalker/no-silver-bullet-de-identification-still-doesnt-work/
#google health
#http://thehealthcareblog.com/blog/2014/07/08/google-co-founders-thanks-but-no-thanks/
#django
#http://www.fullstackpython.com/django.html
#twitter dermatology paper
#http://andrewcantino.com/
#ipython
#ipython v0.13.2 --compile vim with python
#string search
#https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm
#http://blog.phusion.nl/2010/12/06/efficient-substring-searching/


#mcmc
#http://techeffigy.wordpress.com/2014/06/30/markov-chains-explained/
#bayes
#http://www.businessweek.com/articles/2014-07-03/hospitals-are-mining-patients-credit-card-data-to-predict-who-will-get-sick
#http://www.businessweek.com/articles/2014-07-03/hospitals-are-mining-patients-credit-card-data-to-predict-who-will-get-sick
#cc
#http://www.businessweek.com/articles/2014-07-03/hospitals-are-mining-patients-credit-card-data-to-predict-who-will-get-sick

#pandas plots boxplot, trellis, regression
#http://nbviewer.ipython.org/github/fonnesbeck/Bios366/blob/master/notebooks/Section2_7-Plotting-with-Pandas.ipynb

#bayes notebook
#http://nbviewer.ipython.org/github/knathanieltucker/209fp/blob/master/final.ipynb
#numpy
#http://wiki.scipy.org/Tentative_NumPy_Tutorial
#http://nbviewer.ipython.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-2-Numpy.ipynb

#measure theory
#http://blog.echen.me/2011/03/14/laymans-introduction-to-measure-theory/

#xkcd
#http://xkcd.com/793/
#quote/motivation
#elegant_design/tools + try to do it the stupid way = WIN !! <== key to programming

#hash crypto
#https://saadbinakhlaq.wordpress.com/2012/05/23/python-and-cryptography-with-pycrypto/

#compute data
#http://blog.paralleluniverse.co/2013/10/16/spaceships2/
#http://www.paralleluniverse.co/

#rescue plan
#http://arstechnica.com/science/2014/02/the-audacious-rescue-plan-that-might-have-saved-space-shuttle-columbia/

# adaboost 
# http://narrowmark.com/2014/04/03/adaboost/

# -- misc -- ##
## thesis writeup
#https://saadbinakhlaq.wordpress.com/2012/05/12/jinja2/
#https://saadbinakhlaq.wordpress.com/2013/07/25/book-review-building-impressive-presentations-with-impress-js/
# https://readthedocs.org/
# sphinx
#markdown -> web -> https://pythonhosted.org/PyQt-Fit/mod_utils.html#module-pyqt_fit.utils

# -- python notebooks -- #
#odes as markov_chain: http://nbviewer.ipython.org/github/URXtech/techblog/blob/master/continuousTimeMarkovChain/markovChain.ipynb

#data sources: http://rs.io/2014/05/29/list-of-data-sets.html
#cancer project compuational glover virus kill cancer, ecosystem
#http://www.washingtonpost.com/news/morning-mix/wp/2014/05/15/womans-cancer-killed-by-measles-virus-in-unprecedented-trial/?tid=hp_mm

#gaming ouya is good $50
#http://wololo.net/2014/05/16/the-shock-of-playing-the-ouya-one-year-later/

#library books
#https://medium.com/book-excerpts/4ca8405f1e11

#google voice howto
#http://blog.gleitzman.com/post/40774573324/liberating-google-voice-placing-and-receiving-calls
#rubics cube
#http://fulmicoton.com/posts/rubix/
# http://www.sciencenews.org/view/generic/id/345820/description/Delaying_gratification_is_about_worldview_as_much_as_willpower","Delaying gratification is about worldview as much as willpower | Psychology | Science News

#http://puzzlers.org

#auto-poem http://con.puzzlers.org/mainecon/flats

# bias in narrative
#http://www.psmag.com/navigation/books-and-culture/game-telephone-way-hear-enemies-blame-80301/

# security
#http://glenngreenwald.net/pdf/NoPlaceToHide-Documents-Uncompressed.pdf
#surv vs espion : https://www.schneier.com/essay-449.html
#http://www.net-security.org/secworld.php?id=16694
#http://eprint.iacr.org/2014/257.pdf
#https://tails.boum.org/
#heartbleed
#http://blog.meldium.com/home/2014/4/10/testing-for-reverse-heartbleed
#-----------------------------------------------------------------------------------------------------------------<

#---industry------------------------------------------------------------------------------------
#Robot Operating System
#http://www.itworld.com/422034/open-source-robotics-os-moving-lab-farms-and-even-space
#Rethink Robotics
#BlueRiverTechnology lettucebot
#WillowGarage
#avidbots
#http://www.ros.org/:w

#fellowship
#http://www.bayesimpact.org/fellowship
#socrata
#http://www.socrata.com/careers/
#http://careers.stackoverflow.com/jobs/55871/senior-principal-software-data-engineer-dc-audax-health?a=168IKtple
#http://www.crunchboard.com/jobs/index.php
#stratosphere
#http://stratosphere.eu/
#jobs

#leap motion api to track motion ** project idea ?
#using for hand tremor in comments using for neuro company
#http://www.christopherlsmith.com/projectblog/2014/5/13/hand-tremor-analysis

#oscar hacking obamacare 80mln funding
#http://oscarhealth.tumblr.com/

#simplyinsured sanfran ca
#http://www.jobscore.com/jobs2/simplyinsured/software-developer/d9IZfSd10r46BliGakhP3Q?detail=Hacker+News&remail=&rfirst=&rlast=&sid=161

#http://omop.org/TechnologyRequirements

#farm android
#https://farmlogs.com/jobs/

#python security
#http://careers.stackoverflow.com/jobs/54799/android-developer-viaforensics?a=14PF3RDpK

#civis datascience
#https://www.linkedin.com/jobs2/view/13095256?trk=job_view_browse_map

#software-engineering
#books
#https://news.ycombinator.com/item?id=7756497
#howdoi
# howdoi format time in python

#good code to read
#http://docs.python-guide.org/en/latest/writing/reading/
#https://github.com/gleitz/howdoi/blob/master/howdoi/howdoi.py

#'''
#bad: 
#def a(*args):
#	x,y=args	
#	return dict(**locals)
#def b(x,y ):
#	return({'x':x,'y':y}) 
#'''	

#python broad guide
#http://docs.python-guide.org/en/latest/

# optimization
#http://blog.regehr.org/archives/1146

#poetry challenge June 1
# http://con.puzzlers.org/mainecon/flats
#pointer challenge
#https://blogs.oracle.com/ksplice/entry/the_ksplice_pointer_challenge
#generators
#http://stackoverflow.com/questions/17688435/python-how-to-append-generator-iteration-values-to-a-list
#http://ozkatz.github.com/improving-your-python-productivity.html","Improving Your Python Productivity 
#http://www.drewconway.com/zia/?p=1614
#http://www.drewconway.com/zia/?p=204

#>-------------------------------------------------------------------------------------------------<



#----------------
#-- refs --#
#---plotting ---

#princeton python plots
#http://nbviewer.ipython.org/github/PrincetonPy/Python-Workshop/blob/master/3.Demos.ipynb
#boxplot pandas day of week
#http://stackoverflow.com/questions/17194581/best-way-to-generate-day-of-week-boxplots-from-a-pandas-timeseries
#perl  
#http://en.wikipedia.org/wiki/Xmgrace
#**VERY-GOOD matplotlib tutorial/summary: GOTO
#http://www.loria.fr/~rougier/teaching/matplotlib/
#rgba vals:
#http://matplotlib.org/api/colors_api.html
#colors
#http://stackoverflow.com/questions/8931268/using-colormaps-to-set-color-of-line-in-matplotlib
#matplotlib plot
#http://matplotlib.org/1.3.1/users/pyplot_tutorial.html

#gallery
#http://leejjoon.github.io/matplotlib_astronomy_gallery/

# very nice radar type plot
## * http://leejjoon.github.io/matplotlib_astronomy_gallery/cfasurvey/cfasurvey.html

#pendulum animation
#http://www.moorepants.info/blog/npendulum.html

#subplots
#http://nbviewer.ipython.org/github/duartexyz/BMC/blob/master/Ensemble%20average.ipynb

#seaborn light dark
#http://www.stanford.edu/~mwaskom/software/seaborn/tutorial/aesthetics.html

#plot of patients as stick figures:
# http://nbviewer.ipython.org/gist/theandygross/4544012
#nice graphs
#http://www.theswarmlab.com/r-vs-python-round-3/

#story-telling
#https://github.com/LLK/scratch-flash




# -- distributions ---
#failure-rate esimation
#healthyalgorithms.com/2014/05/16/mcmc-in-python-estimating-failure-rates-from-observed-data/
#http://pages.stern.nyu.edu/~adamodar/New_Home_Page/StatFile/statdistns.htm#_ftnref2

#stat reference:
#http://inperc.com/wiki/index.php?title=Elementary_Statistics_by_Bluman

#qq-plot rule out normal
#ks to see if normal, lognormal, weibull, etc
#pass gender grouped
#http://stackoverflow.com/questions/15630647/fitting-lognormal-distribution-using-scipy-vs-matlab?rq=1
#http://stackoverflow.com/questions/8870982/how-do-i-get-a-lognormal-distribution-in-python-with-mu-and-sigma?lq=1
#exploratory:
#http://nbviewer.ipython.org/github/herrfz/dataanalysis/blob/master/week3/exploratory_graphs.ipynb

#---time-series analysis --#
#fitting model to long time series
#http://robjhyndman.com/hyndsight/long-time-series/
#time series cv 
#"http://alumni.cs.ucr.edu/~mvlachos/","Michalis (Michail) Vlachos Homepage","ude.rcu.sc.inmula.",2,0,1,,1669,1369873364885084,"1c1k9c_f4KX9"
#"http://alumni.cs.ucr.edu/~mvlachos/taships.html","academics","ude.rcu.sc.inmula.",1,0,0,,117,1369873313788325,"G8UcT0qnkHAN"

#apache logs of website correlate server_request with traffic
#http://nbviewer.ipython.org/github/koldunovn/nk_public_notebooks/blob/master/Apache_log.ipynb

# sun-spots time normalization
#http://nbviewer.ipython.org/gist/jhemann/4569783

#ordering of time series for ML
#http://stats.stackexchange.com/questions/3337/ordering-of-time-series-for-machine-learning?rq=1
#http://robjhyndman.com/hyndsight/crossvalidation/

#time series regression
#http://statweb.stanford.edu/~jtaylo/courses/stats203/notes/time.series.regression.pdf

#make time stationary
#http://stats.stackexchange.com/questions/2077/how-to-make-a-time-series-stationary?rq=1



# -- alerts---#

'''refs

'''
# A kernel is a type of probability distribution function, required to be even.  A kernel is non-negative, real-valued, even, integral=1.  PDFs which are kernels include uniform(-1,1), and standard normal distributions.
# KDE estimates the pdf of a continuous random variable without assumption about its underlying distribution, non-parametrically.  At every point, a kernel is created with the point at its center; therefore kernel is symmetric.  PDF is then estimated by adding all of the kernel functions and dividing by number of data(non-negative, and normalizes).
'''
#http://archive.today/ulPFkfferings
http://www.dspguide.com/ch9/3.htm
music.pdf benson musical offerings
http://cs.stackexchange.com/questions/12497/what-the-difference-between-the-fourier-transform-of-an-image-and-an-image-histo

Multiplying signals, amplitude modulation

##Discrete Fourier Transform

The Discrete Fourier Transform(DFT) deals with decomposing a signal into a combination of sinusoid coefficients, ordered by frequency.

###Forward Discrete Fourier Transform:

$$X_k = \sum_{n=0}^{N-1}x_n\cdot e^\frac{-i 2\pi k n}{N}$$

###Inverse Discrete Fourier Transform (IDFT):
$$ x_n = \sum_{k=0}^{N-1}X_k e^ \frac{-i 2\pi k n}{N}$$


  The transform from $x_n\rightarrow X_k$  represents a translation from the original, time domain to the frequency domain.  The observed sequence used by DFT is a convolution of an ideal (infinite, periodic) signal with a finite windowing function.  DFT assumes an ideal signal; if the observation time is not an integer multiple of the period, this results in incoherent sampling, causing the frequency components of the signal change.  In dealing with aperiodic signals, where the frequency range is not clear, a signal may change over time and the changes themselves may vary.  

    Convolution in the time domain corresponds to multiplication in the frequency domain; using DFT, however, is faster and easier to interpret.  The frequency response represents the amplitude and phase changes of cosine waves, caused by a linear system; this completely describes the system.  The fourier transform is not sensitive to the amplitude or variance of the original signal, only its frequency. 

	  Finally, the frequency spectrum produced is dependent on windowing size used; in a plot of frequency vs time, a smaller window has more localised time but greater spread in frequency, while larger window sizes better identify true frequency components but are spread more in the time axis.  

	    Therefore, outlier detection using fourier transform requires window size, frequency amplitude, and frequency threshold parameters to be set.  If the frequency response of the signal contains a frequency component greater than the frequency threshold, then the position is labeled an outlier. 

'''
#optimization: http://nbviewer.ipython.org/github/arokem/teach_optimization/blob/master/optimization.ipynb
#minimax:http://stackoverflow.com/questions/7856588/python-minimax-for-tictactoe 
#knapsack: http://bertolami.com/index.php?engine=blog&content=posts&detail=knapsack-problem


a = 12

b = '''
 
### kernel regression
# A kernel is a type of probability distribution function, required to be even.  A kernel is non-negative, real-valued, even, integral=1.  PDFs which are kernels include uniform(-1,1), and standard normal distributions.
# KDE estimates the pdf of a continuous random variable without assumption about its underlying distribution, non-parametrically.  At every point, a kernel is created with the point at its center; therefore kernel is symmetric.  PDF is then estimated by adding all of the kernel functions and dividing by number of data(non-negative, and normalizes).

# The bandwidth of a kernel is the standard deviation.  Chosen small for large, tightly packed data, larger for sparse, small data sets.  

'''
'''
#http://jakevdp.github.io/blog/2013/08/28/understanding-the-fft/

#python module for anomaly detection
#https://github.com/mihaibivol/Graphite-Anomaly-Detector/
#median-deviation
#http://stackoverflow.com/questions/22354094/pythonic-way-of-detecting-outliers-in-one-dimensional-observation-data
#power-law
https://pypi.python.org/pypi/powerlaw
#fft
http://see.stanford.edu/see/courseInfo.aspx?coll=84d174c2-d74f-493d-92ae-c3f45c0ee091
#bayes log-normal model
http://engineering.richrelevance.com/bayesian-ab-testing-with-a-log-normal-model/
'''
#*****
#http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/ 
#*****

#visualize algorithms
#http://bost.ocks.org/mike/algorithms/

#Alerts
# - kernel_reg, change_pt, fft, counsyl, prediction_interval
#otter2tmp
# raw, multivariate, kernreg, fft

#fitting log-normal
#http://stackoverflow.com/questions/8747761/scipy-lognormal-distribution-parameters
#http://stackoverflow.com/questions/8870982/how-do-i-get-a-lognormal-distribution-in-python-with-mu-and-sigma?lq=1

#confidence interval
#http://www.variousconsequences.com/2010/02/visualizing-confidence-intervals.html
#http://stackoverflow.com/questions/18792752/bootstrap-method-and-confidence-interval?rq=1
# fft
#/refs/music.pdf mexican-hat transform
#http://ozkatz.github.com/improving-your-python-productivity.html

# distribution/pareto or percolation
#http://www.liafa.univ-paris-diderot.fr/~labbe/blogue/2012/12/percolation-and-self-avoiding-walks/

#bayesian changepoint
#https://github.com/hildensia/bayesian_changepoint_detection.git
#http://comments.gmane.org/gmane.comp.python.scikit-learn/10332

#event related data analysis

#http://www.pymvpa.org/tutorial_eventrelated.html

#failure rate
#http://healthyalgorithms.com/2014/05/16/mcmc-in-python-estimating-failure-rates-from-observed-data/

#sequential
#http://nbviewer.ipython.org/github/rasbt/algorithms_in_ipython_notebooks/blob/master/ipython_nbs/sequential_selection_algorithms.ipynb?create=1

#powerlaw
#http://nbviewer.ipython.org/gist/jeffalstott/19fcdd6a4ba400ce8de2

# vienese maze
#bostock d3.js maze to tree

#anomoly detection
#http://stats.stackexchange.com/questions/10271/automatic-threshold-determination-for-anomaly-detection?rq=1

#outlier

'''
##318009,"http://stats.stackexchange.com/questions/1142/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series?rq=1","Simple algorithm for online outlier detection of a generic time series - Cross Validated","moc.egnahcxekcats.stats.",0,0,0,4730,45,,"PDGpQ6B2hWZ6"
##318011,"http://stats.stackexchange.com/questions/5700/finding-the-change-point-in-data-from-a-piecewise-linear-function?rq=1","regression - Finding the change point in data from a piecewise linear function - Cross Validated","moc.egnahcxekcats.stats.",0,0,0,4730,45,,"WPYKWjdDaE-u"
##318013,"http://stats.stackexchange.com/questions/35137/appropriate-clustering-techniques-for-temporal-data?lq=1","machine learning - Appropriate clustering techniques for temporal data? - Cross Validated","moc.egnahcxekcats.stats.",0,0,0,4730,45,,"sksniL5X1R6D"
##318019,"http://biomet.oxfordjournals.org/content/92/4/787.abstract","Symmetric diagnostics for the analysis of the residuals in regression models","gro.slanruojdrofxo.temoib.",0,0,0,,45,,"LZEwqHIqyLnv"
##318027,"http://www.originlab.com/www/helponline/origin/en/UserGuide/Graphic_Residual_Analysis.html#Detecting_outliers_by_transforming_residuals","Graphic Residual Analysis","moc.balnigiro.www.",0,0,0,11632,45,,"ywvhk5nvwpT_"

#3d { singlevscum, kernel_reg, change_pt, fft }
#pie graph

#multivariate copulas
http://nbviewer.ipython.org/github/olafSmits/MonteCarloMethodsInFinance/blob/master/Week%209.ipynb?create=1

#clustering
#condesnsation generalized p(x) track in clutter
http://www.robots.ox.ac.uk/~misard/condensation.html
'''

#----model reliability----------------------------------------------------------------------------
#data sources:
#http://apps.who.int/gho/data/?theme=main

#OLS R-language statsmodel
#http://statsmodels.sourceforge.net/devel/examples/notebooks/generated/robust_models_1.html

# http://en.wikipedia.org/wiki/Micromort
#datamining hyperloglog (very good)
#incoming streaming data probabilistic counting by bits not order stats
#http://research.neustar.biz/2012/10/25/sketch-of-the-day-hyperloglog-cornerstone-of-a-big-data-infrastructure/

'''
st like all the other DV sketches, HyperLogLog looks for interesting things in the hashed values of your incoming data.  However, unlike other DV sketches HLL is based on bit pattern observables as opposed to KMV (and others) which are based on order statistics of a stream.  As Flajolet himself states:
'''


#ML application p >> n
#refs/papers/applyingMLtoclnical*.pdf victoria stodden stanford 2008
#http://sparselab.stanford.edu/

#http://nbviewer.ipython.org/github/mwaskom/Psych216/blob/master/week6_tutorial.ipynb
##### http://nbviewer.ipython.org/github/unpingco/Python-for-Signal-Processing/blob/master/Compressive_Sampling.ipynb
#http://nbviewer.ipython.org/url/perrin.dynevor.org/exploring_r_formula_evaluated.ipynb
#survival curves
#http://nbviewer.ipython.org/github/CamDavidsonPilon/lifelines/blob/master/docs/Survival%20Analysis%20intro.ipynb
#psych model weibull distribution**
#http://nbviewer.ipython.org/github/arokem/teach_optimization/blob/master/optimization.ipynb
#image models neuro
#http://nbviewer.ipython.org/github/jonasnick/ReceptiveFields/blob/master/receptiveFields.ipynb

#stat basic stats in pandas
#http://www.randalolson.com/2012/08/06/statistical-analysis-made-easy-in-python/
#http://people.duke.edu/~ccc14/pcfb/analysis.html

#overfitting
#http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/

#R-formula
#refs/imm6614.pdf statsmodel patsy R-formula slide30_34

#---disease model--------------------------------------------------------------------------------------------------
#wilson algo
#random tree generator
#http://bl.ocks.org/mbostock/11357811
#clustering
#http://nbviewer.ipython.org/github/herrfz/dataanalysis/blob/master/week4/clustering_example.ipynb
#http://www.windml.org/examples/visualization/clustering.html

#---bayesian--------------------------------------------------------------------------------------------------------
#http://nbviewer.ipython.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb
# radon example
#http://nbviewer.ipython.org/github/fonnesbeck/multilevel_modeling/blob/master/multilevel_modeling.ipynb
#http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/

'''
http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/

hypoth test(correlation)
1. null  hypoth r=0 (2-sided r neq 0)
2. null distr
3. collect data (r=0.15)
4. calculate pval for data
5. reject/or fail reject null(not enough evidence to reject)
if reject, saying that if that observing random is unlikely given null-true, so its false)

p-val and ci duals

p = effect-size(diff of obsMean from nullmean) / sample size


apply bayes : P(null/data) = P(d|null)P(null) / P(D)
if p<0.05, says when no effect, will see signficant(but really random) 5% of time, so can reject the null, or say not enough evidence to rule out the null

stat power = 1-specificity(typeII error)o
lecture 6 **
report between group differences not within group stanfordmedstatsL6
interaction(cant prove null, can do equivalence trial)

correlation: 
if ignore overestimate within group p-val
underestimate(overly optimistic) p-val between groups
McMemar's exact test, differentiate between 1/0 of each factor

so change level of interaction(ie stores vs people)
GEE modeling, mulivariate regression

time to event model:rate ratio, cox regression, kaplan-meier, frailty model 
### assumptions:
n>30 to 100 assume normal (can do t-test of means)
equality of variances to pool variances(check that groups have equal variances, of p<0.5, then suggest unequal variances)
pooled more precise std(=> t-distribution more dof)
1.define hypoth
2.get null distribution(pool) ie m and w equal at math, same std/var 
(difference of mean= t-distribution for small, z-distr for large)
get se for diff of two means
3.effect size
4.calc p-val
5. 

### for patient data ###
## compar icu to telehealth using paired ttest (L7)
compare mean of the difference between groups
null hypothesis is for average change

## compare more groups, more time points
3 groups: 3 pair-wise test(of mean) increase typeI error
so want to look at all differences at once; by looking at variance
'is difference in mean of groups more than noise(ie var (in group) )
F = var(between-groups)'summarize mean differences'/ var(in-group)'analogous to pooled variances from ttest'
f-test test if two variances are equal(this is null, f close to 1 if true)
anova(f-test) only tells at least 2 groups differ, but not which one

#correct post-hoc
bonferroni
tukey (adjust p)

# repeated measures
sig differences across time periods? significant diff between groups(ctg predictor)? sig diff bewteen groups over time?

#non parametric tests
rank the tests
if null:(no diff) then difference between sum of ranks(series)n*n+1/2, is equal to difference between sums
can get E(u) and -> pvalue

# non-parametric anova (kruskal-wallis) pg112
stas question, do alerts vary by group 

#relative risk between groups
p118
logistic-regression -> adjusted odds ratio
(x2 chi squared) observed-expected/expected  .. effect size

#t-event
event or censoring (time to).. generate survival curve from this
without censoring survivalcurve(kaplan-meier) is just the proportion survive study
variance of chi-square is hypergeometric

#confounding
l8 pg93
#logistic regression -> from betas to odds-ratio
multivariate model testing
'''
'''
Sound It Out:(math-phonics)
ditta
The Probability of A * Probability of new data | A =   P(new data) * P(A|new data)    


'''

#----boosting-------------------------------------------------------------------------------------
#http://www.metacademy.org/graphs/concepts/boosting_as_optimization#focus=boosting_as_optimization&mode=explore
#http://130.203.133.150/viewdoc/summary;jsessionid=86C1563E69B2FB2794C36B6BE961F95F?doi=10.1.1.170.2812 Fast boosting using adversarial bandits
#http://stats.stackexchange.com/questions/ask","Adaboost feature weight calculation - Cross Validated - Stack Exchange
#http://stats.stackexchange.com/questions/40568/adaboost-feature-weight-calculation - Adaboost feature weight calculation - Cross Validated
#http://stats.stackexchange.com/questions/25699/feature-selection-without-target-variable?rq=1","Feature selection without target variable - Cross Validated
#http://www.pnas.org/content/105/39/14790.full","Higher criticism thresholding: Optimal feature selection when useful features are rare and weak
#https://github.com/jbeard4/pySCION/commit/0b09590e8e6561f13e72a874d2ce60c4ed304fb2
#http://stats.stackexchange.com/questions/23382/best-bandit-algorithmmachine learning - Best bandit algorithm? - Cross Validated

#dataming
#http://stats.stackexchange.com/questions/1856/application-of-machine-learning-techniques-in-small-sample-clinical-studies?rq=1
#http://en.wikipedia.org/wiki/Thompson_sampling","Thompson sampling - Wikipedia,
#http://stats.stackexchange.com/questions/10271/automatic-threshold-determination-for-anomaly-detection?rq=1
### sampling
#random forest
#http://blog.echen.me/2011/03/14/laymans-introduction-to-random-forests/
# reservoir sampling
#http://gregable.com/2007/10/reservoir-sampling.html
#http://www.keithschwarz.com/darts-dice-coins/
#secretary problem

#collinearity
#http://learnitdaily.com/six-ways-to-address-collinearity-in-regression-mVodels/

#-------------------------------------------------------------------------------------------------------------

#mimic2
#http://physionet.org/mimic2/UserGuide/node14.html

#-- sequential --




# -- code-snips -- #
#time delta pandas crap rolling groups window

					#print g['tavg'].str.contains('*L')
					#pd.to_datetime(g['tavg'], format='%H:%M:%S')
					#print ' TYPE ** ', type(g['tavg'])
					#gg = g[ g['alert_v'] == 1 ]
					#print '##$#$#type ', type( gg['tavg1'])
					#gg['cumt2'] = pd.rolling_apply( gg['tavg1'].time, 2, np.sum )
					#gg['cumt'] = pd.rolling_sum(gg['tavg'],2).shift(1)
					#gg['cumte'] = gg.apply(lambda x: pd.expanding_sum(x['tavg'], min_periods=2) )
					#gg['cumt'] = gg['tavg'].cumsum() #apply( lambda x: x['tavg'] + x['tavg'].shift(1)[1:] ) 
					#gg['cumt'] = gg['tavg'].values.cumsum() 


					#gg['cumt2'] = pd.rolling_apply( gg['tavg1'].time, 2, np.sum )

					#pd.to_datetime(gg['tavg'], format='%H:%M:%S')
					#gg['cumt2'] = pd.rolling_apply( gg['tavg'].time, 2, np.sum )

					#print 'cumltive: ',gg
					#return gg #gg[gg['tavg'].notnull()]
					#break
#-- code snips --#
#-- _python --#
#structs

#insert into each algo
#class Bunch:
 #   def __init__(self, **kwds):
  #      self.__dict__.update(kwds)
#point = Bunch(datum=y, squared=y*y, coord=x)
# lambda get var from envt created it, not store reference of var
#tracebacks

			#traceback.print_stack()
			#print repr(traceback.extract_stack())
			#print repr(traceback.format_stack()) 
#recursive generators
#http://linuxgazette.net/100/pramode.html
#def boostrs(vc):
#	obsno = len(vc)
#	i=0
#	while T:
#		if i==0:
#			c0=np.random.choice(vc, size=obsno, replace=True )
#			yield c0
#			i = 1;
#			continue
#		c1 = np.random.choice(c0, size=obsno, replace=True )	
#		yield c1
#		c0,c1 = c1, c1.next()
#
#def boostrs2(vc):
#	obsno = len(vc)
#	vc = vc[:] #local copy
#	i=0
#	if i==0:
#		c0=np.random.choice(vc, size=obsno, replace=True )
#		yield c0
#		i = 1;
#	c1 = np.random.choice(c0, size=obsno, replace=True )	
#	yield c1
#	c0,c1 = c1, c1.next()
#
#def boostvc( dt, header, samples=100 ):
#	#1x100 array [mean,std]
#	for vc in header:
#		bvc = 'boost' + vc 
#		bvc = [];
#		for s in range(samples):
#			rs = boostrs2(dt[vc])
#			rsmean = rs.mean() # np.mean(rs, axis=0)
#			rsstd = rs.std() # np.std(rs, axis=0)
#			bvc.append([rsmean, rsstd])
#
#iv6 = ['sys', 'dia', 'hr1', 'ox', 'hr2', 'wht']
#boostvc(th_data, header=iv6)

# - parse time
#--TODO-----
# -- avg time (dt1,dt2,dt3) 
# take timedelta of max3 - min3
# then divide that by 2, add it to min3, convert back to time
##def avgT(x):
##	#time delta object has days
##	return (x['dt1'] + x['dt2'] + x['dt3'] / 3 )
##th_data['avgT'] = th_data.apply(avgT) #, axis=1)
##
##print('$$ ', th_data['avgT'].dytpe )
# -- parse date time
#print 'date ', th_data['dt1'].head()
#th_data['dt1'] = th_data['dt1'].apply( lambda x: dateutil.parser.parse(x ) )
#pd.to_datetime( th_data['dt1'] )
#print ('&&dt ', th_data['dt1'].dtype, type(th_data['dt1']) )
#-----------

#-- _pandas ---
# -- HDFStore
#http://pandas.pydata.org/pandas-docs/dev/cookbook.html#cookbook-hdf

# -- pivot

	#http://stackoverflow.com/questions/18727920/pivoting-pandas-dataframe-assertionerror-index-length-did-not-match-values
	#http://stackoverflow.com/questions/11232275/pandas-pivot-warning-about-repeated-entries-on-index
	#http://stackoverflow.com/questions/23530665/count-the-number-of-observations-that-occur-per-day
#plt.axvline(p1.mean(), color="r", linewidth=3)
#overflowerror: float inf to int
#http://pymotw.com/2/math/
#http://stackoverflow.com/questions/11548005/numpy-or-pandas-keeping-array-type-as-integer-while-having-a-nan-value
#http://stackoverflow.com/questions/13413590/how-to-drop-rows-of-pandas-dataframe-whose-value-of-certain-column-is-nan
#http://stackoverflow.com/questions/12708807/numpy-integer-nan	
#http://stackoverflow.com/questions/13413590/how-to-drop-rows-of-pandas-dataframe-whose-value-of-certain-column-is-nan
#groupby
##.get_group('telehealth')
#booststrapping private
#---private data-----
#bootstrap data make private
#http://healthyalgorithms.com/2013/10/08/statistics-in-python-bootstrap-resampling-with-numpy-and-optionally-pandas/
 
#fatigue overtreatment
 #http://www.healthmetricsandevaluation.org/news-events/seminar/overdiagnosed-making-people-sick-pursuit-health
#-- test merge vitals and metadata
##vtl = pd.DataFrame({'subject_id': range(10), 'vals':np.random.randn(10)})
##gndr = [ random.choice(['M','F']) for i in range(10)]
##sid = range(10)
##meta = pd.DataFrame(gndr, index=sid, columns=['sex'])
##vtl = vtl.join(meta, on='subject_id')

# melt to long format
#th_data = pd.melt(th_data, value_vars=[('sys','dia','hr1','ox','hr2','wht')]) 
#, var_name='vitals', value_name='vals' )

#dataframe
#th_data = pd.DataFrame(out, columns=header)
#th_data.columns = header 

#-- _seaborn ---
#http://stackoverflow.com/questions/22637547/what-was-the-default-color-palette-for-images-in-seaborn-version-0-2
#facets
#g = sns.FacetGrid(tips, row="sex", col="smoker", margin_titles=True, size=2.5)
#g.map(plt.scatter, "total_bill", "tip", color="#334488", edgecolor="white", lw=.5);
#g.set_axis_labels("Total bill (US Dollars)", "Tip");
#g.set(xticks=[10, 30, 50], yticks=[2, 6, 10]);
#g.fig.subplots_adjust(wspace=.02, hspace=.02);

#context
#sns.set_context('talk') #paper,notebook,poster

#color
#sns.set(palette='Set2')
#sns.set_palette("deep", desat=.6)

#sns.set_style("whitegrid")
#data = 1 + np.random.randn(20, 6)
#sns.boxplot(data);
# test statistic: generalized likelihood ratio

#distrbutions

	#sns.distplot( males['wht'].values, kde=False, fit=stats.lognorm );
	#sns.distplot( np.log( males['wht'].values), kde=False, fit=stats.norm );

	#genderex = [ 1,1,1,1,1,0,0,0,0,0 ]
	#hexample = pd.DataFrame({'subject_id': range(10), 'vals':np.random.randn(10), 'gender':genderex})
	#g = sns.FacetGrid(hexample, col="gender")	
	#g.map( sns.distplot, "vals", kde=True, fit=stats.lognorm )
	#print 'gender ', hexample.describe()

#-- _postgresql ---


#-- _sql ---



# -- TODO -- #
'''  
# countdown
a. alr fatigue, real-time threshold
b. biosurveillance as prior art
1. cli: %_method,credible_interval, fft, online(sprt), alert pie{kern-reg, 
2. weights: make red the hard
3. bayes: monyhallgame in d3(betty drawing), kreske, event-model:pr(#)+t.t.event


# parse the telehealth html files
~/confs-papers-sldes/DMHI-2012-current/reports/*.html  (51)
~/confs-papers-sldes/DMHI-2012-current/server-files txt json files incomplete (33)... but may just be due to low data count

~/BOOKS/bk2/BOOKS/kingston082011
# load pandas --wide-format
sex, age, date-time, sys,dia,hr1,ox,hr2,wht 
'''


# -- import -- #
'''https://www.versioneye.com/python/PyQt-Fit/1.1.15
	https://pypi.python.org/pypi/PyQt-Fit

'''
import tables
import pprint
from itertools import izip
from collections import defaultdict, namedtuple
import datetime as dtt
import traceback
import pylab
import math
import pandas as pd
import random
import csv
import dateutil
import os
import numpy as np
from operator import add, sub
from StringIO import StringIO

from scipy import stats
import statsmodels.api as sm
import scipy  
import scikits.bootstrap as bootstrap
from math import log
from scipy.stats import lognorm, norm

import prettytable    
import seaborn as sns
import matplotlib.pyplot as plt
from statsmodels.graphics.gofplots import qqplot
#http://olgabot.github.io/prettyplotlib/


# -- data -- #
# 1.  _telehealth
#otter_data.py (took parse.py->all.csv {web sql queries} -> all.csv) -> dataframe
# ~/project/data/load_data_th/reports2/*.txt csv
header = ['fname','lname', 'dt1', 'sys', 'dia', 'hr1', 'dt2', 'ox', 'hr2', 'dt3', 'wht']
drcty = '/home/solver/project/data/load_data_th/reports2/'

flw = drcty + 'all.csv'
if flw !=1:
	# extract Data: section
	# Meds: Data: Notes:
	start = False
	out = []
	for fls in os.listdir( drcty):
		fls = os.path.join(drcty,fls)
		with open(fls ,'r') as f:
			rdr = csv.reader(f, delimiter='\t')
			for row in rdr: 
				if 'data:' in [r.lower() for r in row]: 
					start = f.tell()
					continue
				if 'notes:' in [r.lower() for r in row]: 
					start = False
				if start:
					out.append(row)
	# csv writer
	with open(flw, 'wb') as csvfile:
		pndwrt = csv.writer(csvfile, delimiter=';')
		pndwrt.writerows(out)
	
# load dataframe from csv
th_data = pd.read_csv(flw, encoding='latin1', names=header, sep=';', parse_dates=True, dayfirst=False, index_col='dt1')
th_data.index.names=['realtime']

# clean data --(null) na
def cleanr(x):
    if x==r"NULL":
        return "NaN"
    else:
        return x
th_data = th_data.applymap(lambda x: cleanr(x))
#any(th_data is "null")
th_data = th_data.dropna()
if any( pd.isnull( th_data) ):
	print 'telehealth not null'

#private (m/f)
from private import private_data
th_data = private_data(th_data)
th_data_stack = th_data.stack()
#clean
vitalmap={'sys':1, 'dia':2 }	

# 2. _mimic2v26
# flatfiles 1 through 6, can import more records at 
# /home/solver/MIMIC-Importer-2.6/
#otter_data.py takes ( parse.py->all.csv {web sql queries} -> all.csv ) -> dataframe
#from otter_data import m2d
#mc_data = m2d()
from mimic_pandas import mimicpostgresql

# a. mimic_pandas.py queries postgresql -> write to csv
drcty2 = '/home/solver/project/data/'
mimicfile = drcty2 + 'mimic2v26_1_6.csv'

# load directly (+ query also)
#mc_data = mimicpostgresql()
if not os.path.exists( mimicfile ) :
	print '\n:::you are querying the database:::\n'
	#dataframe from postgresql
	dt = mimicpostgresql()
	dt.to_csv(mimicfile, sep='\t', encoding='utf-8')
if os.path.exists( mimicfile ):
	print '\nmimic file exists, not querying\n '

# b. load (pre-query) csv to pandas
hdr =['index','subject_id','sex','dob','dod','hospital_expire_flg', \
	         'itemid', 'description', 'charttime', 'realtime', 'value1num',\
	          'value1uom', 'value2num','value2uom']
##MCD##	
tparse = lambda x: pd.to_datetime(x, format='%Y-%m-%d %H:%M:%S')
#mc_data = pd.read_csv("./data/mimic2v26_1_6.csv", names=hdr, skiprows=1, sep='\t') #nrows=1000
mc_data = pd.read_csv("./data/mimic2v26_1_6.csv", names=hdr, skiprows=1, sep='\t', nrows=3000)
#mc_data = pd.read_csv("/home/solver/project/data/mimic2v26_1_6.csv", names=hdr, skiprows=1, sep='\t', nrows=3000)
year = dtt.timedelta(days=365)
def shift(x):
	x=list(x)
	x[0] = '1'
	x[1] = '9'
	x=''.join(x)
	return x
	
mc_data['timeshift'] = mc_data['realtime'].map(lambda x: shift(x) )
mc_data['timeshift'] = mc_data['timeshift'].apply(tparse)    #  lambda x: pd.to_datetime(x) )
mc_data = mc_data.reset_index().set_index('timeshift')

# clean data --(null) na
if any( pd.isnull( mc_data) ):
	print '\nmimic null \n'

mc_data.reset_index(inplace=True)

# 4. bootstrap values for data sharing

# -- summary --- #
# - _time intervals
def census_th(dt):
	#http://stackoverflow.com/questions/18727920/pivoting-pandas-dataframe-assertionerror-index-length-did-not-match-values
	#http://stackoverflow.com/questions/11232275/pandas-pivot-warning-about-repeated-entries-on-index
	#http://stackoverflow.com/questions/23530665/count-the-number-of-observations-that-occur-per-day
	'''
		dt is telehealth long
		plots factorplot with col=frequency and rows=delta percent
		shows frequencies and change in any-values
		mimic is irregular and goes to final reading
	'''
	#input: format the data
	dt = dt.drop(['gender','source'],1)
	###pri('input',dt.head() )

	#counts format
	fr = dt.drop_duplicates(['realtime','subject_id','variable']) 
	fr = fr.set_index(['realtime','subject_id','variable'])
	fr3 = fr.copy()
	fr3 = fr3.unstack() #unstacks variable(last is default) 
	fr3.columns = fr3.columns.droplevel(0) #flattens index
	fr3 = fr3.reset_index()
	fr3['realtime'] = pd.to_datetime( fr3['realtime'] )
	###pri('fr3',fr3.head() )
	print '$$$ dropped', len(dt), len(fr)
	
	#counts
	cn = fr3.copy()
	cn['freq'] = cn.index.map( lambda x: x and 1 ) 
	print 'frequency*** ', cn[['freq']][:10]

	#group by subject_id
	cn = cn.set_index('realtime').groupby(['subject_id']).resample('D',how='count')
	###pri( 'cn',cn.head(10) )
	print 'frequency*** sampled', cn[['freq']][:10]
	cn = cn.unstack()
	#cn = cn.reset_index(drop=True)
	cn = cn.reset_index(drop=True)
	print 'frequency*** ', cn[['freq']][:10]
##
	print('resampled', cn[:10] )
	print '*** len > 3 '	
	print len( cn[cn['freq']>3] ), len(cn)
	print '** cn mean ** '
	print 'mean', cn['freq'].mean()
	print 'cn describe '
	print cn.columns

	#delta (percent change) by subject_id
	#just have to get the value change for HR1
	th= fr3.copy()
	#raw data long form
	#th = dt.copy()
	###pri('fr3\n ', fr3.head(10))
	th = th.set_index('realtime') #.groupby('subject_id')
	###pri('th ', th.head(5) )
	th['percent'] = th['hr2']/th['hr2'].shift(1) - 1 #%shift	
	###pri('percent ', th['percent'][:5] )

	mu = th['percent'].mean()
	std = th['percent'].std()
	sm = mu-std; lg = mu+std
	print 'sm lg ', sm, lg
	chk = lambda x: x<sm and 'small'\
		   			or x>lg and 'large'\
				   	or 'medium'

	th['delta'] = th['percent'].map(chk)
	th=th.reset_index()
	###pri('delta', th.head() )
	th = th.reset_index()
	#print th[th['delta']=='large'][:10]

	#merge on realtime diff lengths
	mrg = pd.merge(cn,th,left_index=True,right_index=True)
	#pri('mrg',mrg.head())
	#print mrg[mrg['delta']=='large'][:5]
	#print('len', len(cn), len(th) )
	#print('len mrg ', len(mrg) )
	print('merge frequency column **&&^% ', mrg['freq'].values[:5] )

	#sns plot ['delta'] ['freq']
	sns.set_palette("deep", desat=.6)
	sns.factorplot("freq", data=mrg,row="delta",\
				margin_titles=True, aspect=3, size=2) #,palette="PuBu") # palette="PuBuGn_d") #, palette="YlGnBu");#"BuPu_d");	
	sns.jointplot('freq', 'percent', mrg, kind="scatter");
	#sns.kdeplot(mrg["freq"]) #, cumulative=True ) #, label='mimic2')
	plt.show()
				#	x_order=list('large','medium','small') )

def census_mmc(dt):
	'''
		dt is telehealth long
		plots factorplot with col=frequency and rows=delta percent
		shows frequencies and change in any-values
		mimic is irregular and goes to final reading
	'''
	#input: format the data
	dt = dt.drop(['gender','source'],1)
	dt = dt.reset_index()
	#pri('input++',dt.head() )


	#counts format
	fr = dt.drop_duplicates(['realtime','subject_id','variable']) 
	fr = fr.set_index(['realtime','subject_id','variable'])
	fr3 = fr.copy()
	fr3 = fr3.unstack() #unstacks variable(last is default) 
	fr3.columns = fr3.columns.droplevel(0) #flattens index
	fr3 = fr3.reset_index()
	fr3['realtime'] = pd.to_datetime( fr3['realtime'] )
	#pri('fr3',fr3.head() )
	#print '$$$ dropped', len(dt), len(fr)
	
	#counts freq
	cn = fr3.copy()
	cn['freq'] = cn.index.map( lambda x: x and 1 ) 
	#group by subject_id
	cn = cn.set_index('realtime').groupby(['subject_id']).resample('D',how='count')
	##pri( 'cn',cn.head(10) )
	cn = cn.unstack()
	cn = cn.reset_index(drop=True)
	##pri('resampled', cn[:10] )
	##print len( cn[cn['freq']>3] ), len(cn)
	##print 'mean', cn['freq'].mean()
	fmean = cn['freq'].mean()

	#delta (percent change) by subject_id
	#just have to get the value change for HR1
	th= fr3.copy()
	#raw data long form
	#th = dt.copy()


	##print('fr3\n ', fr3.head(10))
	th = th.set_index('realtime') #.groupby('subject_id')
	##pri('th ', th.head(5) )
	th['percent'] = th['hr1']/th['hr2'].shift(1) - 1 #%shift	
	##pri('percent ', th['percent'][:5] )

	mu = th['percent'].mean()
	std = th['percent'].std()
	sm = mu-std; lg = mu+std
	##print 'sm lg ', sm, lg
	chk = lambda x: x<sm and 'small'\
		   			or x>lg and 'large'\
				   	or 'medium'

	th['delta'] = th['percent'].map(chk)
	th=th.reset_index()
	##pri('delta', th.head() )
	th = th.reset_index()
	#print th[th['delta']=='large'][:10]

	#merge on realtime diff lengths
	mrg = pd.merge(cn,th,left_index=True,right_index=True)
	#pri('mrg',mrg.head())
	#print mrg[mrg['delta']=='large'][:5]
	#print('len', len(cn), len(th) )
	#print('len mrg ', len(mrg) )
	#print('merge ', mrg['freq'].values[:5] )
	#mrg = mrg.dropna()
	##mrg = mrg[pd.notnull(mrg['freq'])]
	##mrg = mrg[pd.notnull(mrg['percent'])]
	##mrg = mrg[np.isfinite(mrg['freq'])]
	##mrg = mrg[np.isfinite(mrg['percent'])]
	if any( pd.isnull( mrg) ):
		print 'mcd dropped not null'

	#sns plot ['delta'] ['freq']
	#sns.color_palette("Greens_d")
#	sns.factorplot("freq", data=mrg,row="delta",\
#				margin_titles=True, aspect=3, size=2 ) #, palette="Greens_d");#"BuPu_d");	

	###fre = mrg['freq'].values
	###fre.astype(int)
	###if any( pd.isnull(fre) ):
	###	print 'null found here '
	###print 'fre ', type(fre), fre[:3]
	###perc = mrg['percent'].values
	###perc.astype(int)
	###print 'perc ', type(perc)
	###peer = map(lambda x: math.isnan(x) and -99999, perc )
	###percf = [ -99999 if math.isnan(i) else i for i in perc ]
	###if any( pd.isnull(percf)):
	###	print 'null found perc '
	###percf = np.asarray(percf)
	###print 'percf ', type(percf)
	###print percf[:3], fre[:3]
	###mask = np.ma.array(perc,dtype=int)
	###index =pd.isnull(mrg).any(axis=1)
	####index = mrg.isnull().any(axis=1)
	###print 'index ', index[:10]
	###mask[index] = np.ma.masked
	###if any(pd.isnull(mask) ):
	###	print 'mask not working '
	###print mask[:10]	
	#perc = np.array( np.isfinite( mrg['percent'].values ).astype(int) )
	

	#or np.isfinite		
	mrg = mrg.dropna(subset=['freq','percent'] )
	#mrg = mrg[pd.notnull(mrg).any(axis=1)].copy()
	#pri('mrg null?', mrg.head() )
	#index = pd.isnull(mrg['freq']).any(axis=1)
	#print index[:10]
	#print('mrg null', mrg[index].head() )
	if any( pd.isnull(mrg['freq']) ):
		print 'mrg freq contains null'
	elif any( pd.isnull(mrg['percent']) ):
		print 'mrg percent contains null'
	else:
		print 'clear ??'
	
	#joint grid
	g = sns.JointGrid("freq", "percent", mrg , space=0)
	g.plot_marginals(sns.kdeplot, shade=True)
	g.plot_joint(plt.scatter, alpha=.7, marker=".", color="#334488", edgecolor="white", lw=.5)
	#g.annotate(stats.spearmanr, template="{stat} = {val:.3f} (p = {p:.3g})");
	#g.plot(sns.kdeplot, shade=True, cmap="PuBu", n_levels=20);
	#g.plot(sns.jointplot);
	
	#sns.jointplot('freq', 'percent', mrg, kind="hex");
	#cdf
	#sns.color_palette("Set1")
#	sns.kdeplot(mrg['freq'], cumulative=True, label='telehealth')


	#sns.jointplot(mrg['freq'], mrg['percent'],dropna=True ) #, palette="Greens_d");
	#sns.jointplot('freq', 'percent', mrg, dropna=True) #, kind="kde");
	#freq = mrg['freq']
	##sns.kdeplot(freq, cumulative=True ) #, label='mimic2')
	#percent = mrg['percent']
	#sns.jointplot("freq", "percent",mrg, kind="hex",dropna=True) #, palette="Greens_d");
	#sns.jointplot(freq, percent, kind="kde",dropna=True);
	#sns.jointplot("freq", "percent", mrg,dropna=True);
#	sns.kdeplot(freq, percent, shade=True);
	#sns.kdeplot(mrg['freq'],mrg['percent'],shade=True) #,mrg)
#	ax.set_title(r'$Readings/Day : \/\ \mu = %0.0f ,\/\ hr%Change \/\ \mu = %0.0f$' % (fmean ,mu), fontsize=17)
	plt.show()
				#	x_order=list('large','medium','small') )

from itertools import chain
def census_pmf(d2,m2,dlen=13):
	''' delta pmf of final week final week p.19
	http://stackoverflow.com/questions/18727920/pivoting-pandas-dataframe-assertionerror-index-length-did-not-match-values
	# input merge data(long) -> groupby object over subject_id and source -> get hr2 over last week_range
	# the two pmf dicts (lenght is 7days of week)
	'''
	source = ['telehealth']
	th_pmf = defaultdict(list);
	mm_pmf = defaultdict(list);
	for si,s in enumerate(source): 
		if si==0:
			print '-- setting telehealth'
			dt=d2.copy()
			dt = dt[ dt['source']=='telehealth' ]
			#print 'check no mimic', len( dt[ dt['source']=='mimic' ] )

			dt = dt.dropna()
			#print 'check no mimic', len( dt[ dt['source']=='mimic' ] )
			if any( pd.isnull( dt ) ):
				print 'not null'
				##pri('dt dropped na', dt.head() )
			##pri('tele', dt[ dt['variable']=='hr2'][:5] )
			pmf=th_pmf; #set dictionary
		elif si==1: 
			print '-- setting mimic'
			m2 = m2.reset_index()
			m2['realtime'] = pd.to_datetime( m2['realtime'] )
			dt=m2.copy()	
			print 'len ', len(d2[['realtime']])
			dt = dt[ dt['source']=='mimic' ]
			print 'mimic d2 first', len( d2[ d2['source']=='mimic' ] )
			##pri('mimic', dt.head() )
			pmf=mm_pmf #set mimic dictionary

		pri('tele',dt.head() )	
		dt = dt.drop(['gender'],1)
		dt['realtime'] = pd.to_datetime( dt['realtime'] )	
		#print 'time type ', type(dt['realtime'] )
		th = dt.copy()  
		th2 = th.reset_index(drop=True).drop_duplicates(['source','realtime','subject_id','variable']) 

		th2 = th2.set_index(['source','realtime','subject_id','variable'])
		th2 = th2.unstack()
		print 'unstack worked ** '
		th2.columns = th2.columns.droplevel(0)
		th2 = th2.reset_index()
		pri('unstacked th', th2.head() )

		#calculate percentages
		th2['percent'] = th2['hr2']/th2['hr2'].shift(1) - 1 #%shift	

		# select last week
		#g = th2.groupby(['subject_id','source'])
		if si==0:
			g = th2.groupby(['subject_id'])
			print 'telehealth group'
		elif si==1:
			g=th2

		lastwk = g.apply(lambda x: x[ x['realtime'] > (x['realtime'].iloc[-1] - dtt.timedelta(dlen)) ])
		last = lastwk[['realtime','percent']]
		#print last.head(10)

		# -- deprecated --
		#function to apply to last cross-section 
		#def daynumber(x,srcdict=teleweek):	
		# 	delta = dtt.timedelta(days =1)
		#	day0 = x.applymap(lambda x: x['realtime'].iat[0])
		#	day1 = x.applymap(lambda x: x['realtime'].iat[-1])
		#	week = zip(day0,day1)
		#	for s,e in week:
		#		d=0
		#		while s < e:
		#			#get all row at day
		#			val = last[ last['realtime']==s ]
		#			vals = val['hr2'].values
		#			#h1 = np.histogram( vals, normed=0)[0] / float( len(vals) )
		#			srcdict[d].append(vals)
		#			s += delta; 
		#			d += 1
		
		#http://stackoverflow.com/questions/14734533/how-to-access-pandas-groupby-dataframe-by-key
		#http://stackoverflow.com/questions/11720334/selecting-data-from-pandas-panel-with-multiindex
		##### groupby creates key:val for that groups unique 
		# -- xs just selects a subset, no changes to key,val grouping
		# --  groupby by level, key=level, val=sub select
		try:
#			###tle = last.xs(s,level=1) #level 1 = 'source' s=telehealth or mimic
			###tle = tle.groupby(level=0) #level=0 is subject_id ;  
			tle = last.groupby(level=0)
		except(KeyError):
			print 'source(mimic) not found'
			break;

		#create pmfs for each day of week	
		for k,v in tle:
			#k is subject id
			v['realtime'] = v['realtime'].map(lambda x:x.date() )
			wkds = v['realtime'][:]    #[0]#,v[1][:] #grouped items are 
			wkset = set(wkds)
	
			week = v['realtime'][-1]
			delta = dtt.timedelta(days=1)
			w = week - (delta*dlen)
			deltas = [week - dtt.timedelta(days=i) for i in reversed(range(0,dlen+1)) ]
			dmap = dict( zip(deltas,range(0,len(deltas)) ))
			#print 'the weekset ', wkset
			#print 'tvals ', v['hr2'].values
			for w in wkset:
				#print 'w ',k, w
				vitals = v[ v['realtime']==w ]['percent'].values
				#print 'vitals ', vitals
				#print dmap[w]
				pmf[ dmap[w] ].append(vitals)
		print( '***telehealth dict', th_pmf[6])
	return th_pmf

def histopmf(thpmf, mpmf, bins=14 ):
	#generate histogram 
#	http://stackoverflow.com/questions/11750276/matplotlib-how-to-convert-a-histogram-to-a-discrete-probability-mass-function
	#http://stackoverflow.com/questions/19584029/plotting-histograms-from-grouped-data-in-a-pandas-dataframe
	#print 'th ', thpmf[0][:3]
	#print 'mm ', mpmf[0][:3]
	#print 'len mpmf ', len(mpmf), len(mpmf[0] )
	#h1 = [ float(len(thpmf[i])/len(thpmf))   for i in range(bins) ]
	#h2 = [ float(len(mpmf[i])/len(mpmf))   for i in range(bins) ]
	##print 'h1 ',h1
	##print 'h2 ',h2
	#h1 = np.asarray(h1)
	#h2 = np.asarray(h2)
	def k(d):
		return chain(*d.values())
	t2=[] #np.array(range(6))
	#vec=[]
	for k,v in thpmf.items(): 
		vv = np.fromiter(chain.from_iterable(v), dtype=float )
		tvv=np.median( vv)
		t2.append(tvv)
	#mmvec=[]
	m2=[] #np.array(range(6))
	for k,v in mpmf.items(): 
		vv = np.fromiter(chain.from_iterable(v), dtype=float )
		mvv=np.median( vv)
		m2.append(mvv)
	print '%%%%%%%%%%%%%%'
	print t2, m2
	print len(t2), len(m2)
	#t2=vec
	#m2=mmvec
	#lent2 = [ len(t) for t in t2]	
	#print lent2,sum(lent2)
	#h1=np.histogram(t2,bins=7, normed=1)[0] #/float(len(t2))
	#h2=np.histogram(m2,bins=7, normed=1)[0] #/float(len(m2))
	#print 'histos ', h1[:10], h2[:10]
	ha =[ 100*(a-b) if abs(a)>.01 and abs(b)>.01 else 0.01 for (a,b) in zip(t2,m2)]
	haha =[ 100*(a-b) for (a,b) in zip(t2,m2)]
	hb =[ 100*(b-a) for (a,b) in zip(t2,m2) if abs(a) > .015 and abs(b) > .015 ]
	#hb =[ 100*(b-a) for (a,b) in zip(t2,m2) ]
	ab = np.concatenate( (t2,m2),axis=0 )
	#print len(m2), len(t2)
	#plt.hist(h2,bins=20,normed=0)
	#plt.hist(h1,normed=0)
	##plt.hist(hb,bins=6,normed=0)
	print hb, len(hb)
	days = range(len(haha))

	fig, ax = plt.subplots(1, 1, figsize=(6, 5))
	plt.bar(days,haha,align='center')
	#plt.bar(days,ha,align='center')
	ax.set_xlabel("final_2weeks");ax.set_ylabel("median_%change");
	ax.set_title(r'Heart_Rate Difference (TH - Mimic2)', fontsize=18)
	plt.show()
	#pmft = tele.hist[0]/len(tele)
	#pmfm = mimi.hist[0]/len(mimi)
	#print 'pmf', pmfm[:5]

def census_mimic_pmf(dt1,dlen=13):
	''' the mimic pmf weekly
	'''
	''' delta pmf of final week final week p.19
	http://stackoverflow.com/questions/18727920/pivoting-pandas-dataframe-assertionerror-index-length-did-not-match-values
	# input mimic data(MCD) -> groupby object over subject_id and source -> get hr2 over last week_range
	'''
	#input: format the data
	#dt1.
	dt1['timeshift'] = pd.to_datetime( dt1['timeshift'] )
	#print 'timeshift type', type(dt1.timeshift), dt1.timeshift.apply(type)
	dt = dt1.reset_index(drop=True).copy()
	dt = dt.drop(['level_0','source','index','gender','realtime'],1)
	#pri('input++',dt.head() )
	#pri('hr1', dt[dt.variable=='hr1'][:5] )

	#flatten long table
	#fr1 = dt.reset_index(drop=True)
	#dt3 = dt.reset_index(drop=True)
	fr = dt.reset_index(drop=True).drop_duplicates(['timeshift','subject_id','variable']) 
	fr = fr.set_index(['subject_id','timeshift','variable'])
	fr = fr.unstack() 
	fr.columns = fr.columns.droplevel(0) 
	fr = fr.reset_index() #if no columns then just pass to self
	fr.dropna(subset=['hr2'],inplace=True)
	#pri('fr',fr.head() )

	#calculate percentages
	fr['percent'] = fr['hr2']/fr['hr2'].shift(1) - 1 #%shift	

	#group last week
	#na groups auto excluded by groupby
	g = fr.groupby(['subject_id'])
	#pri( 'g',g.head(2))
	lastwk = g.apply(lambda x: x[ x['timeshift'] > (x['timeshift'].iloc[-1] - dtt.timedelta(dlen)) ])
	#print('lwk', lastwk[:5])
	last = lastwk[['timeshift','percent']]
	#last = last.reset_index(drop=True)
	#pri('last ', last.head())

	tle = last.groupby(level=0)
	#tle = last.reset_index()
	#tle = last.droplevel(0)
	#pri( 'tle check ', tle.head())
	#create pmfs for each day of week	
	pmf = defaultdict(list)
	for k,v in tle:
		#k is subject id
		v['timeshift'] = v['timeshift'].map(lambda x:x.date() )
		wkds = v['timeshift'][:]    #[0]#,v[1][:] #grouped items are 
		wkset = set(wkds)
	
		week = v['timeshift'][-1]
		delta = dtt.timedelta(days=1)
		w = week - (delta*dlen)
		deltas = [week - dtt.timedelta(days=i) for i in reversed(range(0,dlen+1)) ]
		dmap = dict( zip(deltas,range(0,len(deltas)) ))
		for w in wkset:
			#print 'w ',k, w
			vitals = v[ v['timeshift']==w ]['percent'].values
			#print 'vitals ', vitals
			#print dmap[w]
			pmf[ dmap[w] ].append(vitals)
	print( '!!! mimic dict', pmf[6][:10])
	return pmf

#http://wesmckinney.com/blog/?tag=pandas box-plots
#http://nbviewer.ipython.org/github/dolaameng/tutorials/blob/master/exploratory-data-analysis/PLOT%20-%20seaborn%20tutorial%201.ipynb
def census_box(dt):
	'''boxplots:
	average frequency; length of stay; sex,age,geography; 
	[ sex, age, geography ]
	[ lenght_of_stay, avg_frequency_day]
	'''
	#input thmi merged
	dt.reset_index(drop=True, inplace=True)
	# age, do not have telehealth ages
	pri('thmi', dt.head() )
	#dt = dt[ pd.notnull(dt.dob) ]; 
	def birthdeath(b,d):
		'''>90 set to 90'''
		try:
			bint = int(b[:4] ) #bd.map(lambda x: int(x[:4]) ) 
			dint = int(d[:4] ) #dd.map(lambda x: int(x[:4]) )
			age  = dint - bint
			return age
		except:
			pass
	sid = dt.groupby(['subject_id']).first()
	sid['age'] = np.vectorize(birthdeath)( sid['dob'], sid['dod'] ) 
	sid = sid.reset_index(drop=True) 
	pri('age ',sid.head() )

	def weighted_choice(choices):
	   total = sum(w for c, w in enumerate(choices) )
	   r = random.uniform(0, total)
	   upto = 0
	   for c, w in enumerate( choices):
	      if upto + w > r:
	         return c
	      upto += w
	   assert False, "Shouldn't get here"
	
	def geo(x):
		l = ['urban','sub','rural']
		p = dict(enumerate(l) )
		c = [2,2,1]
		n = weighted_choice(c)
		return p[n]
	sid['geo'] = sid.index.map(lambda x: geo(x)) 
	pri('geo',sid.geo.head() )
	
	agelist = ['<40','40-60','>60']
	aged = dict(enumerate(agelist) )
	sid['ageg'] = sid.age.map(lambda x: x<40 and aged[0]\
									or x>60 and aged[2]\
									or aged[1] )

	#sid['geoC'] = sid.groupby(['geo']).size()
	#sid['ageC'] = sid['age'].mean()
	#sid['genderC'] = sid.groupby(['gender']).size()
	#pri('sidC', sid[['gender','genderC']][:10] )

	xtab = pd.crosstab(sid.source, [sid.gender,sid.geo,sid.ageg] ) #,sid.geo], rownames=['source'], colnames=['s','g'])
	print 'xtab', xtab.head()
	#geo, age, gender 
	#can create group over column
	f, ax = plt.subplots() #1,2, figsize=(4, 4), sharey=True)
	sns.set_style("darkgrid", {"grid.linewidth": .5, "axes.facecolor": ".9",'xtick.direction': '45'}) #, 'xtick.major.size': 0, 'xtick.minor.size': 0,  })
	#f.set_xticklabels(rotation=10)
	sns.boxplot(xtab,color='pastel')
	#sns.boxplot(sid.loc[:, 'age'], groupby=[sid.gender,sid.ageg])
	#sns.boxplot(sid.loc[:, 'age'], groupby=[sid.gender,sid.geo ])
	#sns.boxplot(sid.loc[:, 'genderC'], groupby=sid.source)
	#sns.boxplot(sid.loc[:, 'geoC'], groupby=sid.source)
	#sns.boxplot(sid.loc[ : ['age']], groubpy=sid.source, color="pastel")
	#sns.boxplot(sid, names=['age', groupby=sid.source)
	#sns.factorplot("ageg",data=sid,col='gender', hue='source',kind='box')
	#sns.factorplot('source',sid ,kind='box')
	#sns.factorplot("gender","genderC","source",sid ,kind='box')
	f.tight_layout()
	#ax.set_xticklabels(,rotation=45 )
	#range(N), rotation=45, fontsize=8)
	ax.set_title('Gender,Geography,Age', fontsize=25)
	plt.show()


def time_norm(dt):
	'''thinkstats.pg74 mean testing
	'''

	#input: format the data input is long
#subject_id |   source   |         realtime        | variable | value 
	dt = dt.drop(['gender','index'],1)
	dt = dt.reset_index()
	##pri('input++',dt.head() )
	#counts format
	dt = dt.drop_duplicates(['realtime','source','subject_id','variable']) 
	dt = dt.set_index(['realtime','source','subject_id','value','variable'])
	##pri('dropped', fr.head() )
	fr3 = dt.copy()
	fr3 = fr3.unstack() #unstacks variable(last is default) 
	fr3.columns = fr3.columns.droplevel(0) #flattens index
	fr3 = fr3.reset_index()
	fr3['realtime'] = pd.to_datetime( fr3['realtime'] )
	#pri('fr3',fr3.head() )
	##counts freq
	cn = fr3.copy()
	cn['freq'] = cn.index.map( lambda x: x and 1 ) 
	#group by subject_id, source
	cn = cn.set_index('realtime').groupby(['source','subject_id']).resample('D',how='count')
	##pri( 'cn',cn.head(10) )
	exit(0)
	cn = cn.unstack()
	cn = cn.reset_index(drop=True)
	##pri('resampled', cn[:10] )
	##print len( cn[cn['freq']>3] ), len(cn)
	##print 'mean', cn['freq'].mean()
	fmean = cn['freq'].mean()
	exit(0)

	#overflow int error
	mrg = mrg.dropna(subset=['freq'] )
	if any( pd.isnull(mrg['freq']) ):
		print 'mrg freq contains null'
	else:
		print 'clear ?'

	#groupby source
	src = mrg.groupby(['source'])['frequency']

	#kdeplot


def timeplotH(dt, title='mimic'):
	#group subject_id
	# -- reset index
	dt.reset_index(inplace=True)
	dt['realtime'] = dt['realtime'].apply(pd.to_datetime )   
	#print 'type ', dt.dtypes 
	#print type(dt.realtime)

	# --  min, max timestamps
	grpt = dt.groupby('subject_id')['realtime']
	subj_min  = grpt.min() ; 
	subj_max  =  grpt.max() ;
	descr = grpt.describe() #df['delta'] = (df['tvalue']-df['tvalue'].shift()).fillna(0)
	
	# subject min max 
	mn = subj_max - subj_min
	intdy = mn.map(lambda x: int(x.days) < 7 and 7 or int(x.days) < 365 and int(x.days) or int(0) )
	
	#mean stats
	mm = mn.map(lambda x: int(x.days) )
	mu = mm.mean()
	ms = mm.std()
	#print('day as int**: ', mm, mu, ms )
	#print('#deltaT', mn) #print('days ', mn.map(type) ) 
	
	#global min max
	# -- y is number of subjects, 
	y = list( xrange( dt.subject_id.nunique() ) )
	y=np.asarray(y)
	print ' y type ', type(y)
	# -- x is days int 
	mmin = dt.realtime.min();
	mmax = dt.realtime.max()
	mlen = mmax - mmin;
	totdys = int(mlen.days)
	
	#plot model:: [mmin ..  (min ..  x1/2 .. max) .. mmax]
	subrange = subj_min - mmin
	subrdays = subrange.map(lambda x: int(x.days) )   #print('##! ', len(subrdays), subrdays[:10]) 
	x5raw = subrdays + intdy/2
	x5raw.sort()
	x5 = x5raw.map(lambda x: x%365 )  #int(x) < 365 and int(x) or int(365/2))
	
	# plot dashed year-lines
	# -- use x5raw as no of days -> year 
	x55 = x5raw.copy()
	
	#chop up the x55 days
	x55.sort() 
	xdz = [ d/365 for d in x55]
	yrs = []; xz =0
	for i,x in enumerate(xdz):
		if x != xz:
			yrs.append(i)
			xz=xz+1
		
	#print x55
	#print('###chopped ', xdz )
	#print('###years ', yrs )

	#gender
	#yf = dt[dt['gender']=='Female']
	##yf = list( xrange( yf.subject_id.nunique() ) )
	##yf = np.asarray(yf)

	#print 'yf ',len(yf) #yf[:10]
	#print ' ## x5 ', x5[:10], x5.values[:10], len(x5)
	#print ' ## y ' , y[:10], len(y)
	#set female index to its number
	#findex = [ i for i,v in enumerate(y[female]) if v ]
	#x5v = x5.values
	#print '$x5 vals ', x5v[:10], x5v[female][:10]
	#fval = [ v for i,v in enumerate(x5v[female]) if v ]
	#print '\nfindex, fval ', findex[:10], fval[:10] 

	# plot
	sns.axes_style("darkgrid")
	sns.set_palette("deep", desat=.6)
	
	fig, ax = plt.subplots()
	ax.errorbar( x5, y, xerr=intdy, fmt='ok', ecolor='grey',elinewidth=2, alpha=0.4)
#	ax.errorbar( fval, findex, xerr=intdy, fmt='ok', ecolor='seagreen',elinewidth=2, alpha=0.4)
#	ax.plot( x5, y, marker='o', color='seagreen',alpha=0.2)
	ax.set_xlabel("365_days, day_range");ax.set_ylabel("subjects incr_by_year");
	ax.set_title(r'$%s \/\ days : \/\ \mu = %0.0f ,\/\ \sigma = %0.0f$' % (title, mu , ms), fontsize=25)
	ax.set_xlim(0,400 )
	plt.show()
	#for i,x in enumerate(xdz):
	#   ax.plot( [0,i] , [365,i], 'k_', lw=1 )
	#   ax.text(1, -0.6, r'$\sum_{i=0}^\infty x_i$', fontsize=20)
	#for i,y in enumerate(yrs):
	#	ax.text(-0.1, y, 'year%d' % i, fontsize=12) # ha='center', va='center')	
	#plt.axis=((0,500,0,45))
	#fig.autofmt_xdate()
	
# - normal vs lognormal
#def vitalsdistplot(dt):
#	''' male female log-norm fit of vitals '''
#	#g = sns.FacetGrid(tips, row="sex",col="itemid" hue="itemid")
#	#g.map(
#	pass

# - _distributions FacetGrid
def distributionsFG(dt,row="gender",col="variable", val="value"):
	g = sns.FacetGrid(dt, row=row, hue="gender",col=col, margin_titles=True, xlim=(0,300), ylim=(0,0.05), despine=True )
	g.map( sns.distplot, val , kde=False, fit=stats.norm, 
			kde_kws={"color": "seagreen", "lw": 3, "label": "KDE"}, 
			hist_kws={"histtype": "stepfilled","alpha":0.7},
			fit_kws={"color":".2", "lw": 3}
			); 
	plt.show()
	#stats.pearsonr
	#g.map(sns.kde
	#g.set_axis_labels("Total bill (US Dollars)", "Tip");
	#g.set(xticks=[10, 30, 50], yticks=[2, 6, 10]);
	#g.fig.subplots_adjust(wspace=.02, hspace=.02);

	#malesgrp = males.groupby('subject_id')['whtlog'].values
	#print 'data ', maleid19['wht'].values
	#sns.kdeplot(maleid19['wht'].values)
#	log
#	#100x27
#	smp = [  [w[ np.random.randint(1,len(w)) ] for (w) in malesgrp ] for i in range(100) ] 
#	print 'smple ', len(smp)
#	smple = np.asarray( smp)
#	print 'type *', type(smple)#, smple.dtypes

def lognormal(x,sex='male'):
	''' fits data to log-distribution, with fixed parameters '''
	#1.explicit(take log(x) and get m,s); 2.fit a normal distribution to log(x); 3.fit x to lognormal distribution
	#ddof=1 uncensored, uses unbiased estimator(expected err=0)
	x = x['wht'].values
	def lognfit(x, ddof=0):
		x = np.asarray(x)
		logx = np.log(x)
		mu = logx.mean()
		sig = logx.std(ddof=ddof)
		return mu, sig
	#lognorm
	mean, stddev = lognfit(x)
	print '** std, men **', stddev, mean
	dist = lognorm([stddev],loc=mean)
	#fit
	shape,loc, scale = lognorm.fit(x, floc=0)
	d2 = lognorm([scale], loc=loc)
	#plot
	fig, ax = plt.subplots(1, 1, figsize=(6, 5))
	xi=np.linspace(0,10,5000)

	#fit
	plt.plot(xi,d2.pdf(xi), label='log norm fitted')
	
	#original
	ln = len(x)+1
	#n,bins,patches = 
	plt.hist( np.log(x) , ln, normed=1, label='sample log(x)' ) 
	#np.log(x).hist(bins=ln, ax=ax, alpha=.5,normed=1,label='sample,log(x)' )
	plt.plot(xi,dist.pdf(xi), label='lognorm pdf')
	plt.plot(xi,dist.cdf(xi), label='lognorm cdf')

	#title
	ax.set_xlim(4,8 )
	ax.set_title(r"$lognormal \/\ distribution \/\ weight \/\ %s$" % sex,fontsize=18)
	plt.legend(loc='upper right')
	ax.set_ylabel('frequency log(x)')	
	ax.set_xlabel(r'log(x) n= %d' % ln, fontsize=12)
#	ax.set_title(r'$%s \/\ days : \/\ \mu = %0.0f ,\/\ \sigma = %0.0f$' % (title, mu , ms), fontsize=25)
	plt.show()

def logfitcdf( dt ):
	'''
		plots distribution, and fits
	   	plots cdf and fit
	'''
	#sns.axes_style("darkgrid")
	sns.set_palette("deep", desat=.6)
	#sns.set_context(rc={"figure.figsize": (200, 3)})
	sns.set()
	#current_palette = sns.color_palette("coolwarm", 7)
	#sns.set_style("whitegrid")
	#sns.set_context("notebook")
	#sns.color_palette("muted", 8)
	#sns.set_style("dark")
	#sns.despine(left=True)

	#data
	dtraw = dt.copy().values.flatten()
	dtlog = dt['wht'].apply(lambda x:np.log(x) ).values.flatten()

	# Set up the plots
	f, (ax1, ax2, ax3) = plt.subplots(1,3) 
	c1, c2, c3 = sns.color_palette("dark", 4)[:3]	

	#linear
	maxd = dtraw.max()
	bins = np.linspace(0,maxd, maxd+1)
	ax1.hist(dtraw, bins/4, normed=True, alpha=0.5, histtype="stepfilled")

	shape, loc, scale = stats.lognorm.fit(dtraw, floc=0) 
	mu = np.log(scale) 	# Mean of log(X)
	sigma = shape 		# Standard deviation of log(X)
	M = np.exp(mu) 		# Geometric mean == median
	s = np.exp(sigma) 	# Geometric standard deviation

	x = np.linspace( dtraw.min(), dtraw.max(), dtraw.max()+1 )
	ax1.plot(x, stats.lognorm.pdf(x, shape, loc=0, scale=scale), linewidth=3) 

	ax1.set_xlabel('weight(lbs)')
	ax1.set_ylabel('P(x)')
	ax1.set_title('Linear')
	#leg=ax1.legend()	

	#log
	maxl = dtlog.max()
	bins = np.linspace(0,maxl, maxl+1)
	ax2.hist(dtlog, bins*3, normed=True,alpha=0.3, histtype="stepfilled")

	shape2, loc2, scale2 = stats.lognorm.fit(dtlog, floc=0) 
	mu = np.log(scale)  # Mean of log(X)
	sigma = shape 		# Standard deviation of log(X)
	M = np.exp(mu) 		# Geometric mean == median
	s = np.exp(sigma) 	# Geometric standard deviation

	x = np.linspace(dtlog.min(), dtlog.max(), num=400)
	ax2.plot(x, stats.lognorm.pdf(x, shape2, loc=0, scale=scale2), linewidth=3 ) 
	#sns.kdeplot(dtlog,shade=True,ax=ax2)
	ax2.set_xlabel('log weight(lbs)')
	ax2.set_ylabel('P(x)')
	ax2.set_title('Logs')

	#cdf
	ecdf = sm.distributions.ECDF(dtlog)
	x = np.linspace(min(dtlog ), max(dtlog ))
	y = ecdf(x)
	ax3.step(x, y)
	shape, loc, scale = stats.lognorm.fit(dtlog, floc=0) 
	ax3.plot(x,stats.lognorm.cdf(x, shape, loc=0, scale=scale), linewidth=3)
	
	ax3.set_xlabel('log weight(lbs)')
	ax3.set_ylabel('$\sum P(x)$')
	ax3.set_title('Cumulative')
	f.tight_layout()
	plt.show()

def logfitcdfold( dt ):
	'''
		plots distribution, and fits
	   	plots cdf and fit
	'''
	#data
	#sns.axes_style("darkgrid")
	sns.set()
	current_palette = sns.color_palette("coolwarm", 7)
	
	sns.set_style("whitegrid")
	sns.set_context("notebook")
	#sns.color_palette("muted", 8)
	#sns.set_style("dark")
	#sns.despine(left=True)
	dtraw = dt.copy().values.flatten()
	dtlog = dt['wht'].apply(lambda x:np.log(x) ).values.flatten()
	
	#plt.figure( figsize=(12,4.5))
	#raw: hist + fit
	ax1 = plt.subplot(131)
	n, bins, patches = plt.hist(dtraw, bins=100, normed=True)
	#dtraw.hist(bins=100, alpha=.5,label='all male',normed=1) #,cumulative=True)

	shape, loc, scale = stats.lognorm.fit(dtraw, floc=0) # Fit a curve to the variates
	mu = np.log(scale) # Mean of log(X)
	sigma = shape # Standard deviation of log(X)
	M = np.exp(mu) # Geometric mean == median
	s = np.exp(sigma) # Geometric standard deviation

	x = np.linspace(dtraw.min(), dtraw.max(), num=400)
	plt.plot(x, stats.lognorm.pdf(x, shape, loc=0, scale=scale), linewidth=2 )
	ax = plt.gca() # Get axis handle for text positioning
	txt = plt.text(0.9, 0.9, 'M = %.2f\ns = %.2f' % (M, s), horizontalalignment='right', 
	                size='large', verticalalignment='top', transform=ax.transAxes)	
	#plt.xlim(0,150)
	plt.xlabel('weight(lbs)')
	plt.ylabel('P(x)')
	plt.title('Linear')
	leg=ax1.legend()	

	#log
	ax1 = plt.subplot(132)
	n, bins, patches = plt.hist(dtlog, bins=100, normed=True)
	#dtlog.hist(bins=100, alpha=.5,label='all male',normed=1) #,cumulative=True)

	shape, loc, scale = stats.lognorm.fit(dtlog, floc=0) # Fit a curve to the variates
	mu = np.log(scale) # Mean of log(X)
	sigma = shape # Standard deviation of log(X)
	M = np.exp(mu) # Geometric mean == median
	s = np.exp(sigma) # Geometric standard deviation

	x = np.linspace(dtlog.min(), dtlog.max(), num=400)
	plt.plot(x, stats.lognorm.pdf(x, shape, loc=0, scale=scale), 'b', linewidth=2 ) 
	ax = plt.gca() # Get axis handle for text positioning
	txt = plt.text(0.9, 0.9, 'M = %.2f\ns = %.2f' % (M, s), horizontalalignment='right', 
	                size='large', verticalalignment='top', transform=ax.transAxes)	
	#plt.xlim(0,150)
	plt.xlabel('log weight(lbs)')
	plt.ylabel('P(x)')
	plt.title('Logs')
	leg=ax1.legend()	

	#cdf
	ax2 = plt.subplot(133)
	ecdf = sm.distributions.ECDF(dtlog)
	x = np.linspace(min(dtlog ), max(dtlog ))
	y = ecdf(x)
	ax2.step(x, y)
	shape, loc, scale = stats.lognorm.fit(dtlog, floc=0) 
	ax2.plot(x,stats.lognorm.cdf(x, shape, loc=0, scale=scale), '--', linewidth=2)
	
	plt.xlabel('log weight(lbs)')
	plt.ylabel('$\sum P(x)$')
	plt.title('Cumulative')

	plt.show()

	#x = np.linspace(lognorm.ppf(0.01, s), lognorm.ppf(0.99, s), 100)
	#plt.plot(x,cum,'r--')
	#plt.plot(x, lognorm.pdf(x, s), 'r-', lw=5, alpha=0.6, label='lognorm pdf')
	#rv = lognorm(s)
	#plt.plot(x, rv.cdf(x), 'k-', lw=2, label='frozen pdf')

def normalfit(x):
	#normV
	x = x['wht'].values
	nm, ns = norm.fit(x)
	print '** std, men norm **', nm, ns, x[:3]
	#normdist = norm([ns],loc=nm)

	#plot
	ln = len(x)+1
	fig, ax = plt.subplots(1, 1, figsize=(6, 5))
	xi=np.linspace(0,200,500)
	plt.hist( x , normed=1, alpha=0.3 ) 
	#x.hist(bins=ln, ax=ax, alpha=.5,normed=1,label='sample,x' )
	plt.plot(xi,norm.pdf(xi,loc=nm,scale=ns), label='norm fitted')
	#plt.plot(xi,norm.pdf(xi), label='norm original')
	
	ax.set_title("fit normal distribution",fontsize=18)
	plt.legend(loc='upper left')
	plt.show()

def qqlog(x):
	'''log-norm take log(x)
		#http://stats.stackexchange.com/questions/77752/how-to-check-if-my-data-fits-log-normal-distribution
	'''
	x = x['wht'].values
	x = np.log(x)
	print 'x ** ', x
	qqplot( x , dist=stats.t,line='45', fit=True);
	pylab.show()
def qqnorm(x):
	'''gauss.norm , ks-stat'''
	x = x['wht'].values
	print 'x ** ', x
	qqplot( x , line='45', fit=True);
	pylab.show()
	
		
# - _jet plot (time ensemble)
def ensembleaverage(dt):
	'''
	time normalized
	http://nbviewer.ipython.org/github/duartexyz/BMC/blob/master/Ensemble%20average.ipynb
	'''

# - box-plot {age, pt, demographic, time}

# - alerts --#

# - _rugplot, 

# - _exp_cdf(arrival time)

# - _CI
# Confidence interval is the mean,variance over a population; so that given 100 random samples and a 95%CI, 5 CIs should be expected to not contain the normal mean(0) variance(1).  Prediction interval is the probability the next point is within the population mean,variance. 

def confidenceinterval(dt):
	'''
	#http://statsmodels.sourceforge.net/devel/examples/generated/example_ols.html	
	#http://nbviewer.ipython.org/github/duartexyz/BMC/blob/master/ConfidencePredictionIntervals.ipynb
	calculate all samples mean, std
	calculate ci for 1 sample group by subject_id
	
	lambda apply to dt['alertci'] if not contain mean, then alert
	count number of alerts as tuple for each 	

	if deviate plot as red
	#variables
	- inputvectors6 1D-array ['sys', 'dia', 'hr1', 'ox', 'hr2', 'wht']
	- runfunction 
	- output plot
	- join axis
	'''
	iv6 = ['sys', 'dia', 'hr1', 'ox', 'hr2', 'wht']
	iv6count = [ i + 'cicount' for i in iv6 ]
	print iv6, iv6count
	vitalcols = [ dt[c] for c in iv6 ]
	#countcols = [ dt[c] for c in iv6count  ]

	# -- splat-unpack	
	#alertcount = NamedTuple('alertcount', [subject_id, dataindex] + iv6 )
	#index=12, dtp_val = [1,0,0,1,1,1]; subjid = 13456; 
	#alertcount(subjid, index, *dtp_val) 

	out = []
	# iv6, iv6count are just list of name-strings
	for vc,cnt in zip(iv6,iv6count):
		print '?? val cols ', dt[vc].head()
		#population
		n = dt['subject_id'].count()      # total number of observations, all subject_id
		M = dt[vc].mean() 
		S = dt[vc].std()  
		T = stats.t.ppf(.975, n-1)        # T statistic for 95% and n-1 degrees of freedom

		#groupby subject_id
		x = dt.groupby('subject_id')[vc]  # 1 subject_id
		m = x.mean()      
		s = x.std()		  				  # panda default axis=0, ddof=1  
	
		#confidence interval given population parameters
		# -- column name variables
		vcz = vc + 'z'		#z-normalized columns
		vcs = vc + 'cisub'; vca = vc + 'ciadd'; #ci add subtract columns
		vco = vc + 'outlier'

		# -- z-transform mean=0 variance=1
		dt[vcz] = dt[vc].map( lambda x: (x- M ) / S)
		Mz = dt[vcz].mean()
		Sz = dt[vcz].std()

		def ci(arr, op):
			rhs = arr * T / np.sqrt(n)
			return np.array( op(Mz, rhs) ) 

		dt[ vcs ] = dt[[vcz]].apply( ci, axis=1,op=sub )
		dt[ vca ] = dt[[vcz]].apply( ci, axis=1,op=add )

		 # CIs that don't contain the true mean
		def cint(x):
			return 1 if x[vcs]*x[vca] > 0 else  0
		#app2 = lambda x,y: x*y >0 and 1 or 0
		dt[vco] = dt.apply(cint,axis=1)

		#print '\n z-norm ', dt[vcz ][:3]
		#print '\n confidence interval ', dt[vcs ][:3]
		#print '\n confidence interval ', dt[vca ][:3]
		#print '\noutliers ', dt[ dt[vco] == 1]
			
		#bootstrap ci
		#http://stats.stackexchange.com/questions/18396/determining-the-confidence-interval-for-a-non-normal-distribution
#http://stackoverflow.com/questions/16707141/python-estimating-regression-parameter-confidence-intervals-with-scikits-boots
#http://stats.stackexchange.com/questions/92209/can-i-pull-a-confidence-interval-out-of-a-single-sample-by-dividing-it-into-sub
		#el = sm.emplike.DescStat(dt[vc])
		#print 'ci ', el.ci_mean()
		# tibshirani resample with replacement at same sample size
		vcob = vco + 'b'
		CI = bootstrap.ci(dt[vcz], scipy.mean, alpha=0.10 ) #, n_samples=10000)
		print '\n bootstrapped 90%', CI[0], CI[1], len(CI), CI
		dt[vc+'ci'] = [ 1 if i>CI[0] and i<CI[1] else 0 for i in dt[vcz] ]
		print 'dtvc ', dt[vc+'ci'][:10], '\n ', dt[vc][:10], len( dt[dt[vc+'ci']==1])/len(dt) * 100.0

	#plot	--dt['whtci']	
	fig, ax = plt.subplots(1, 1, figsize=(13, 5))
	ind = np.arange(1, 101)
	ax.axhline(y=0, xmin=0, xmax=n+1, color=[0, 0, 0])
	ax.plot([ind, ind], CI[:100], color=[0, 0.2, 0.8, 0.8], marker='_', ms=0, linewidth=3)
	ax.plot([ind[out], ind[out]], ci[:, out], color=[1, 0, 0, 0.8], marker='_', ms=0, linewidth=3)
	#ax.plot([ind[out], ind[out]], ci[:, out], color=[1, 0, 0, 0.8], marker='_', ms=0, linewidth=3)
	ax.plot(ind, m, color=[0, .8, .2, .8], marker='.', ms=10, linestyle='')
	ax.set_xlim(0, 101)
	ax.set_ylim(-1.1, 1.1)
	ax.set_title("Confidence interval for the samples' mean estimate of a population ~ $N(0, 1)$",
             fontsize=18)
	ax.set_xlabel('Sample (with %d observations)' %n, fontsize=18)
	plt.show()
	
#confidenceinterval(th_data)


#bootstrap procedure vs t-test
#*** very good ! http://courses.washington.edu/matlab1/Bootstrap_examples.html
#http://www.stat.umn.edu/geyer/5601/examp/
#prediction interval vs confidence interval:
#**S.Thrun stat lecture on CI
''' ci is population(within) pi is p(next) <t+1> in interval '''

def boostci(dt, samples=100):
	'''generates 100 bootstrap samples from observations'''
	'''returns frame with columns=[mean,std,vitals,cip,cis]'''
	''' ci is based on percent cutoffs, so ['ci'] just shows range '''
	''' to normalize use T-stat  #T = stats.t.ppf(.975, n-1)a '''
	iv6 = ['sys', 'dia', 'hr1', 'ox', 'hr2', 'wht']
	dtlen = len(dt)	

	for v,vc in enumerate(iv6):
		### dt-frame columns ### 
		bci = 'boostci'+vc #mean std
		bco = 'boostoutlier' + vc #1 if not in mean	
		
		#obs boostraped over 1000 samples 
		meantemp=[];stdtemp=[] 
		for i in xrange(samples):
			if i == 0:
 	   			c0 = np.random.choice(dt[vc], size=dtlen, replace=True)
				c1 = c0
	 	   		meantemp.append( np.mean(c0,dtype=np.float64) )
				stdtemp.append( np.std(c0, dtype=np.float64) ) 
 	   			continue;

 	   		c0,c1 = c1, np.random.choice(c0, size=dtlen, replace=True)
 	   		meantemp.append( np.mean(c1,dtype=np.float64) )
			stdtemp.append(  np.std(c1, dtype=np.float64) )

		#ci
		ciptemp = np.add( meantemp,np.sqrt(stdtemp)) 		
		cistemp = np.subtract( meantemp,np.sqrt(stdtemp)) 		
	
		#ci(95%) drawn from empirical_distribution(samples=1000) using \
		#t-distr, to norm the samples'(e.d) std(+-),samples'(e.d) mean 
		n = samples 
		T = stats.t.ppf(.975, n-1)
		print '**T ', T
		Mz=meantemp
		#def cif(arr, op=sub):
		#	rhs = arr * T / np.sqrt(n)
		#	return np.array( op(Mz, rhs) ) 
		#def cif2(mn, op):
		#m,s = meantemp,stdtemp
		#ci = m + np.array([-s*T/np.sqrt(n), s*T/np.sqrt(n)])
		#out = ci[0, :]*ci[1, :] > 0       # CIs that don't contain the true mean
		#print 'out ', out
		#tstatsub 
		
			
		#tstatsub = apply(cif,meantemp,axis=1,op=sub )
		#tstatadd = apply(cif,meantemp,axis=1,op=add )
		#tstatplus = meantemp.apply( cif, axis=1,op=add )
				

		#DataFrame object: columns=mean, std, vital: create/append
		#'tip':tstatplus, 'tis':tstatsub,
		if v==0:
			vc = [vc for i in xrange(samples) ] #gen 100 vcs downcolumn
			dtboost =pd.DataFrame({'mean':meantemp,'std': stdtemp, 
									'vitals':vc,  								 							   'cip':ciptemp, 'cis':cistemp, 
								})
		elif v>0:
			print 'VVV ', v
			vc = [vc for i in xrange(samples) ] #gen 100 vcs downcolumn
			new =pd.DataFrame({'mean':meantemp,'std': stdtemp, 
									'vitals':vc,  								 							   'cip':ciptemp, 'cis':cistemp, 
								})
			dtboost = pd.concat([dtboost,new], ignore_index=True)

	print 'dtboost frame ', dtboost.head(), dtboost.tail(), dtboost.describe()
	return dtboost

def logboostci(dt, samples=100):
  	''' generates 100 bootstrap samples from observations
		** 1.log(x) : 2.B[mean +- T*std] 3.vs normal_mean
		1.x : 2.b[log(mean) +- log(T)*std] 3.vs mean
	#log-normal is not log of normal, but distribution whose log is normal
	#log-mean no mu, exp(mu+std^2/2)
	http://nbviewer.ipython.org/url/xweb.geos.ed.ac.uk/~jsteven5/blog/lognormal_distributions.ipynb
	'''
	dtlen= len(dt)	
	vc ='wht'
	whtv = dt[vc].values

	#z-transform
	mm = np.mean(whtv,axis=0)
	ss = np.std(whtv,axis=0)
	def z(x):
		return (x-mm)/(2.0 * ss)
	whtvz=[z(x) for x in whtv]
	#print 'log z ', whtv, whtvz, mm,ss

	#log(x)
	logwht = [ np.log(x) for x in whtv ]
	#print '**log t ', dt.head(),'\n', vc2.head()
	#print 'log wht ', logwht
	
	#bootstrap 
	meantemp=[];
	stdtemp=[] 
	for i in xrange(samples):
		if i == 0:
 			c0 = np.random.choice(logwht, size=dtlen, replace=True)
			c1 = c0
	   		meantemp.append( np.mean(c0,dtype=np.float64) )
			stdtemp.append( np.std(c0, dtype=np.float64) ) 
 			continue;

 		c0,c1 = c1, np.random.choice(c0, size=dtlen, replace=True)
 		meantemp.append( np.mean(c1,dtype=np.float64) )
		stdtemp.append(  np.std(c1, dtype=np.float64) )

	
	#ci(95%) 
	n = samples 
	t1 = stats.t.ppf(.975, n-1)
	T = stats.t.ppf(.975, n-1,loc=mm,scale=ss)
	#print '**T ',t1, T, np.log(T)
	#T = np.log(T)
	T = t1

	m=np.array(meantemp) / log(mm) 
	s=np.array(stdtemp)	/ log(ss) 
	#print 'm,s ** ','\n', m[:3], '\n', s[:3], 
	#m,s=meantemp,stdtemp
	#ci = m + np.array([-s*T/np.sqrt(n), s*T/np.sqrt(n)])
	cis = m - s*T/np.sqrt(n)
	cip = m + s*T/np.sqrt(n)
	ci = np.array([cis,cip])
	#print '\nci\n',ci
	out = ci[0, :]*ci[1, :] > 0       # CIs that don't contain the true mean
	ot = cip-cis
	#out = np.power(cip-cis,2) > 0
	print 'out ', ot[:3]
	
	#plot
	fig, ax = plt.subplots(1, 1, figsize=(13, 5))
	ind = np.arange(1, 101)
	ax.axhline(y=0, xmin=0, xmax=n+1, color=[0, 0, 0])
	ax.plot([ind, ind], ci, color='grey', marker='_', ms=0, linewidth=2)
	ax.plot([ind[out], ind[out]], ci[:, out], color=[1, 0, 0, 0.8], marker='_', ms=0, linewidth=3)
	ax.plot(ind, m, color='grey', marker='.', ms=10, linestyle='')
	#ax.set_xlim(0, 101)
	#ax.set_ylim(-1.1, 1.1)
	ax.set_title("Confidence interval: t-test, log(x), bootstrap$",
	             fontsize=18)
	ax.set_xlabel('Sample (with %d bootstrap)' %n, fontsize=18)
	plt.show()
	
#scipy
#http://pages.physics.cornell.edu/~myers/teaching/ComputationalMethods/python/arrays.html
def test(dt):
	''' input is mean(logx) for each subject split by gender'''
	''' get mu,sigma from lognorm fit of data'''
	whtv = dt
	print 'whtv ', len(whtv), whtv.shape

	#GEOMETRIC MEAN exp() anti-log
	MN = np.exp( np.mean(whtv )) #geometric mean,std
	STD = np.std( whtv )
	print 'globals ', MN, STD

	#fit frozen distribution
	dist=lognorm([STD],loc=MN)

	#samples #100x27
	#log( lognorm-x ) fits a normal distribution
	n = 100 ; ls=27  
	#m =  np.log( np.mean( whtv,axis=1 ) )
	#s =  np.log( np.std( whtv, axis=1, ddof=1) )
	m = np.mean( whtv, axis=1 )
	s = np.std( whtv, axis=1, ddof=1)
	print 'std ', s[:3]
#shape
	T = stats.t.ppf(.975, n-1,loc=MN,scale=STD)        # T statistic for 95% and n-1 degrees of freedom
	t2 = stats.t.ppf(.975, n-1 )        # T statistic for 95% and n-1 degrees of freedom
	print 'T ', T, t2
	#T=t2
	#T = np.exp(T)
	ci = np.exp( m + np.array([-s*T/np.sqrt(n), s*T/np.sqrt(n)]) )
	#ci = np.array([m/(s**2), m*(s**2)])
	out = ci[0, :]*ci[1, :] > 0       # CIs that don't contain the true meano
	fig, ax = plt.subplots(1, 1, figsize=(13, 5))
	ind = np.arange(1, n+1)
	ax.plot(ind, dist.pdf(ind),color=[0,0,0] )
	ax.axhline(y=0, xmin=0, xmax=n+1, color=[0, 0, 0])
	ax.plot([ind, ind], ci, color=[0, 0.2, 0.8, 0.8], marker='_', ms=0, linewidth=3)
	ax.plot([ind[out], ind[out]], ci[:, out], color=[1, 0, 0, 0.8], marker='_', ms=0, linewidth=3)
	ax.plot(ind, m, color=[0, .8, .2, .8], marker='.', ms=10, linestyle='')
	ax.set_xlim(0, n+1)
	#ax.set_ylim(-1.1, 1.1)
	ax.set_title("Confidence interval for log ~$logN$",
			             fontsize=18)
	ax.set_xlabel('Sample (with %d observations)' %n, fontsize=18)
	plt.show()

## percent_method
#http://www.uvm.edu/~dhowell/StatPages/Resampling/BootstCorr/bootstrapping_correlations.html
#if time-normalization possible, invariant to it Effron

#a data-object with only 1 vital(grouped) is passed
def percent_method2( dtb1 ):
	'''sorts means, takes top/bottom 5% as cutoff, compute CI-range
	'''
	dtb = dtb1[['mean','cip','cis','std','vitals']].copy()

	#normalize to [-1 to 1] #sort ascending
	#zscore = lambda x: (x-x.mean() ) / (2.0 * x.std())	
	#dtb = dtb[['mean','cip','cis','std']].apply(zscore)
	dtb = dtb.sort(['mean']) #ascending 

	#raw-sizes
	size = int( len(dtb) )
	print 'size ', size
	praw = int( len(dtb)*0.03 )  
	qraw = len(dtb) - praw

	#index
	index = dtb[['mean']].index 
	dms = dtb[['mean','cip','cis','std']]
	bottom = index[:praw]
	top = index[qraw:]
	mdl = index[praw:qraw]

	#row-selects top bottom middle 
	tix = dms.ix[ top ]
	bix = dms.ix[ bottom ]
	mrand = np.random.choice( mdl, size=len(mdl), replace=False ) 
	mix = dms.ix[ mrand ]

	#append 1 for color outliers percent method
	pl = pd.concat([bix,mix,tix], axis=0) 
	dms['clr'] = dms.index.map(lambda x: x in mdl and 'NaN' or 1)
	print 'dms ', dms.head(), dms.describe()

	
	#normalize to 100 samples
	norm = len(dtb) / 100
	p = int( (len(dtb)*0.03) / norm ) ; q = 100 - p  #25
	sz = 100 - (2*p)
	b= np.random.choice( bottom, size=p, replace=False)
	m= np.random.choice( mdl, size=sz, replace=False)
	t= np.random.choice( top, size=p, replace=False)
	bi,mi,ti = dms.ix[b], dms.ix[m], dms.ix[t]
	ndt = pd.concat([bi,mi,ti],axis=0)

	#shuffle
	print 'p q ', p, q
	print ndt.head(), ndt.tail()
	print ndt.describe()
	return ndt

def ttestboost(dtb1):
	'''
	#t-stat means test(may not be same as %pm) over the empirical dist.
	#std of empirical distribution
	#looking at sample means(ms), and adding the %T(p-val from population)
	#* the std (which is the subset_sample_ mean)
	#test at mean=0 cross 
	sample mean estimate of population mean
	'''
	dtb = dtb1[['mean','cip','cis','std','vitals']].copy()
	#grab 100
	m100 = np.random.choice(dtb['mean'].values ,size=100, replace=False)
	s100 = np.random.choice(dtb['std'].values ,size=100, replace=False)

	#z-normalize the e.d. means and stds
	mm = [m100.mean(),s100.mean()]
	ss = [m100.std(),s100.std()]
	def zscore(x, vl=0):
		return (log(x)-mm[vl] ) / (2*ss[vl])	
	def zscore2(x, vl=1):
		return (log(x)-mm[vl] ) / (2*ss[vl])	
	#m100 = dtb[['mean']].apply(zscore)
	#s100 = dtb[['std']].apply(zscore)
	m100 = [ zscore(i,vl=0) for i in m100]
	s100 = [zscore2(i,vl=1) for i in s100]
	print 'lens %%% ', len(dtb), len(m100), len(s100)	
	print 'weird ', m100[:5], s100[:5]
	
	#raw-sizes
	n = 100
	#m = dtb['mean'].values #the means of e.d
	#s = dtb['std'].values #the std of e.d.
	m = m100
	s = s100
	print '%%%555 ', m[:3], s[:3]
	T = stats.t.ppf(.975, n-1)

	#ci at T% for p-counts at n-d.o.f
	#ci = m + np.array([-s*T/np.sqrt(n), s*T/np.sqrt(n)])
	#out = ci[0, :]*ci[1, :] > 0   # CIs that don't contain the true mean
	cip = m + s*T/np.sqrt(n)
	cis = m - s*T/np.sqrt(n)
	out = cip*cis > 0   # CIs that don't contain the true mean
	ci = np.array([ cis,cip ])
	#ci = zip( cis, cip )
	#print 'out!! ', m100[:5], s100[:5], out[:10], len(out)
	print 'ci ** ', cip[:5], out[:5], ci[:3]
	fig, ax = plt.subplots(1, 1, figsize=(13, 5))
	ind = np.arange(1, 101)
	ax.axhline(y=0, xmin=0, xmax=n+1, color=[0, 0, 0])
	ax.plot([ind, ind], ci, color='grey', marker='_', ms=0, linewidth=2)
	ax.plot([ind[out], ind[out]], ci[:, out], color=[1, 0, 0, 0.8], marker='_', ms=0, linewidth=2)
	ax.plot(ind, m, color='grey', marker='.', ms=10, linestyle='')
	#ax.set_xlim(0, 101)
	#ax.set_ylim(-1.1, 1.1)
	ax.set_title("Confidence interval for the samples' mean estimate of a population ~ $N(0, 1)$",
	             fontsize=18)
	ax.set_xlabel('Sample (with %d observations)' %n, fontsize=18)
	plt.show()


def boots( th_data, dtb ):
	''' simple bootstrap over sys '''
	#bootstrap
	print 'dtv 78', th_data.head()
	CIs = bootstrap.ci(data=th_data['sys'], statfunction=scipy.mean)
	print 'bootstrapped CIs ', CIs
	#grab 100
	cis100 = np.random.choice(dtb['cis'].values ,size=100, replace=False)
	cip100 = np.random.choice(dtb['cip'].values ,size=100, replace=False)

	print 'dtv ', dtb.head()
	#plot
	out = dtb['mean'].map(lambda x: (x>CIs[1] and 1 or x<CIs[0]) and 1 )
	
	n=len(th_data['sys'])
	ci = np.array([ cis100, cip100 ])
	print 'ci ', ci[:5]
	m = dtb['mean']
	print 'out ', dtb['mean'].head(5), out
	fig, ax = plt.subplots(1, 1, figsize=(13, 5))
	ind = np.arange(1, 101 )
	#ax.axhline(y=0, xmin=0, xmax=n+1, color=[0, 0, 0])
	# reg bars
	ax.plot([ind, ind], ci, color='grey', marker='_', ms=0, linewidth=2)
	# outlier
	#ax.plot([ind[out], ind[out]], ci[:, out], color=[1, 0, 0, 0.8], marker='_', ms=0, linewidth=2)
	# means
	ax.plot(ind, m, color='grey', marker='.', ms=10, linestyle='')
	#ax.set_xlim(0, 101)
	#ax.set_ylim(-1.1, 1.1)
	ax.set_title("Confidence interval for the samples' mean estimate of a population ~ $N(0, 1)$",
	             fontsize=18)
	ax.set_xlabel('Sample (with %d observations)' %n, fontsize=18)
	plt.show()


import matplotlib.colors as cl
def boostpercentplot( dt ): 
	'''plots the %-method of boosted samples'''
	'''frame columns = mean, cip, cis, std, clr'''
	dt = dt.reset_index()
	#shuffle
	print dt.head()
	#get outlier indexes
	ot = dt[dt['clr']==1]
	print 'out ', ot.index
	out = ot.index

	#sample m,ci	
	ci = np.array([ dt['cis'],dt['cip'] ])
	m = dt[['mean']]

	fig, ax = plt.subplots(1, 1, figsize=(13, 5))
	ind = np.arange(1, 101)
	#ax.axhline(y=0, xmin=0, xmax=n+1, color=[0, 0, 0])
	ax.plot([ind, ind], ci, color='grey', marker='_', ms=0, linewidth=2)
	ax.plot([ind[out], ind[out]], ci[:, out], color=[1, 0, 0, 0.8], marker='_', ms=0, linewidth=2)
	ax.plot(ind, m, color='grey', marker='.', ms=10, linestyle='')
	#ax.set_xlim(0, 101)
	#ax.set_ylim(-1.1, 1.1)
	#ax.set_title("Confidence Interval: boost percent method (norm invariant)$",fontsize=18)
	#ax.set_xlabel('Sample (with %d observations)' %n, fontsize=18)
	plt.show()

#	#normalize
#	M = dt['mean'].mean()
#	S = dt['std'].std() 
#	cmin = dt['cis'].min()
#	cmax = dt['cis'].max()
#	rng = cmax - cmin
#	print ' $$ ', rng
#	#rmin = dt['cis'
#	s=dt['std'].std()
#	# to normalize find the mean, and then get difference from each x
#	# divide by range of delta-x's
#	def normrange(x):
#		M = x['mean'].mean()
#		delta = x['mean'] - M
#		dmin = delta.min() ; dmax = delta.max()
#		drange = dmax-dmin / 2	
#		dnrm = delta / drange
	#print 'ss %% ', ss
	#dt = dt[['mean','cis','cip']].apply(lambda x:x-M/rng)
	#dt['mean'] = dt[['mean']].apply(lambda x: x-M )
	#dt['cis'] = dt[['cis']].apply(lambda x: x-S/S )
	#dt['cip'] = dt[['cip']].apply(lambda x: x-S )
	
	#nrm = lambda x: (x-M)/S
	#dt = dt[['mean','cis','cip']].apply( nrm )
	#dt['cis','cip'] = dt[['cis','cip']].apply( nrm )
	#dt['mean'] = dt[['mean']].apply( nrm )
	#dt['cis'] = dt[['cis']].apply(lambda x: x-M/S)
	#dt['cip'] = dt[['cip']].apply(lambda x: x-M/S)
	#def nrmm(dx):
	#	return ( (dx['mean']-M )	/ dx['std'] )
	#dt = dt[['mean','cis','cip']].apply(lambda x: nrmm(x) )

def percent_method( empirical_distribution ):
	'''sorts dtboost.bci and takes top/bottom 5%'''
	# sort e.d.
	edsor = empirical_distribution[:,0].sort()
	# 95%, 2.5% top bottom
	l = len(edsor)
	tindex = l - l*.025  
	bindex = l*.025
	
	# indexes
	btm = [ i for i,ms in enumerate(edsor) if i < bindex ]
	top = [ i for i,ms in enumerate(edsor) if i > tindex ]
	mdl = [ m for m,ms in enumerate(edsor) if m > bindex and m < tindex ]
	
	# select 100 samples with 5 out 95
	top5 = lambda x: random.choice(edsor[ [btm], [top] ])
	md95 = lambda x: random.choice(edsor[[ mdl ]] )
	a100 = [ top5(i) if i<5 else md95(i)   for i in xrange(100) ] 
	#random.shuffle(a100)
	# ~confidence interval as mean+-std
	ci = edsor[0,:] + (-edsor[1,:], edsor[1,:] )
	print 'ci ', ci
	return ci

def toyplotpm(boostci):
	# plot 100 total from empirical_distribution
	n=100; ind = ed[:]
	fig, ax = plt.subplots(1, 1, figsize=(13,5))
	ind = np.arange(1, 101)
	ax.axhline(y=0, xmin=0, xmax=n+1, color=[0, 0, 0])
	ax.plot([ind, ind], ci[:100], color=[0, 0.2, 0.8, 0.8], marker='_', ms=0, linewidth=3)
	ax.plot([ind[out], ind[out]], ci[:, out], color=[1, 0, 0, 0.8], marker='_', ms=0, linewidth=3)
	ax.plot(ind, m, color=[0, .8, .2, .8], marker='.', ms=10, linestyle='')
	ax.set_xlim(0, 101)
	ax.set_ylim(-1.1, 1.1)
	ax.set_title("Bootstrap Confidence Interval percentile method", fontsize=18)
	ax.set_xlabel('100 Samples' %n, fontsize=18)
	plt.show()

def radarplot():
	#http://www.loria.fr/~rougier/teaching/matplotlib/#d-plots
	axes([0,0,1,1])
	N = 20
	theta = np.arange(0.0, 2*np.pi, 2*np.pi/N)
	radii = 10*np.random.rand(N)
	width = np.pi/4*np.random.rand(N)
	bars = bar(theta, radii, width=width, bottom=0.0)
	
	for r,bar in zip(radii, bars):
	    bar.set_facecolor( cm.jet(r/10.))
	    bar.set_alpha(0.5)

	show()	

def predictioninterval(dt):
	'''
	ensembleaverage http://nbviewer.ipython.org/github/duartexyz/BMC/blob/master/Ensemble%20average.ipynb
	time normalize to percent cycle, interpolate
	lambda apply to dt['ea']
	plot all samples
	plot normalized
	plot ensemble

	can use duartexyz or seaborn
	''' 

import ellipsoid     
def multivariate_prediction_interval(dt):
	''' 
	--bivariate(ellipse) --trivariate(ellipsoid)
	#http://nbviewer.ipython.org/github/duartexyz/BMC/blob/master/PredictionEllipseEllipsoid.ipynb
	#http://stats.stackexchange.com/questions/29860/confidence-interval-of-multivariate-gaussian-distribution
	'''

#multivariate_prediction_interval(th_data)	


# -- _fft -- #
def fft(dth, dtm, thmi):
	''' imperative approach
	'''
	##pri('dt fft', dth.head() )	  
	##pri('dt fft', dtm.tail() )	
	##raw vs norm HR2 
	def cleandatall(dth,vital='hr1'):
	  	dth = dth.drop(['gender'],1)
		th = dth.copy( )  
		th2 = th.reset_index(drop=True).drop_duplicates(['source','subject_id','realtime','variable']) 
		th2 = th2.set_index(['source','subject_id','realtime','variable'])
		th2 = th2.unstack()
		th2.columns = th2.columns.droplevel(0)
		th2 = th2.reset_index()
		mu = th2[vital].mean(); 
		std = th2[vital].std() 
		#mmax = mu+std; mmin= mu-std;
		return th2[[vital]].values, mu, std

	def cleandata(dth,vital='hr1'):
	  	dth = dth.drop(['gender'],1)
		th = dth.copy()  
		th2 = th.reset_index(drop=True).drop_duplicates(['source','subject_id','realtime','variable']) 
		th2 = th2.set_index(['source','subject_id','realtime','variable'])
		th2 = th2.unstack()
		th2.columns = th2.columns.droplevel(0)
		th2 = th2.reset_index()
		mu = th2.hr1.mean(); 
		std = th2.hr1.std() 
		mmax = mu+std; mmin= mu-std;

		#grouby by subject_id
		g = th2.groupby( ['subject_id'] )
		sidgroup = [ v for k,v in g ]
		signal = []
		r=[] #0=raw #1=norm
		for i,s in enumerate(sidgroup):
			sid1     = []
			raw = np.fromiter(chain.from_iterable( s[[vital]].values ), dtype=float )
			sid1.append( raw )
			
			mu  = s[[vital]].mean() 
			std = s[[vital]].std() 
			mmin = mu-std; mmax = mu+std

			sid1.append( mu )
			sid1.append( std )
			def nf(x):
				if x>mmin & x<mmax:
					return x	
			flt = s[(s[[vital]] > mmin) & (s[[vital]] < mmax) ]
			norm = flt[ pd.notnull( flt[vital])  ][vital].values
			#norm = s.where( s>0, s['hr1'], axis='index')
			sid1.append( norm )
			r.append( sid1 )
			#0=raw, 1=mu, 2=std, 3=norm
		return r

	def cleanmimicall( dtm, vital='hr2' ):
		#long data just select
		mc = dtm.copy()
		hr = mc[ mc['variable']==vital]

	def cleanmimic( dtm, vital='hr2' ):
		#long data just select
		mc = dtm.copy()
		hr = mc[ mc['variable']==vital]
		#group by subject_id
		hr_group = hr.groupby('subject_id')
		r = [] 
		for i,s in hr_group:
			tmp =[]
			mu = s.value.mean()
			std = s.value.std()
			mx = mu+std
			mn = mu-std
			v = s.value.values
			norm = [ vv for vv in v if vv>mn and vv<mx ]
			tmp.append(v)
			tmp.append(mu)
			tmp.append(std)
			tmp.append(norm)
			r.append(tmp)
		#0=raw 1=mu 2=std 3=norm
		return r

	#s   = cleandata(dth)
	#s1  = cleanmimic(dtm)
	frc,fra,pltsize = 0.1, 0.01, 1000	
	def total(frc=frc, fra=fra, pltsize=pltsize):
		#count iterator
		fftc=Count()
		vitals = ['sys', 'dia', 'hr1', 'ox', 'hr2', 'wht']
		op=[]
		opindex=[]
		oraw = []
		ostats=[]
		for v in vitals:
			s,mu,std  = cleandatall(dth,vital=v) #select each vital
			ostats.append([mu,std])
			raw = np.fromiter(chain.from_iterable( s ), dtype=float ) #flatten array
			raw = raw[:pltsize]
			s1 = cleanmimicall(dtm, vital=v)
			#raw = np.concatenate( (s[:][0], s1[:][0] ) ) #, axis=0  )
			#raw = np.concatenate( (s, s1 ) ) #, axis=0  )
			#raw = s[0]
			#norm= np.concatenate( (s[5][3], s1[5][3] ) )

			def detect_outlier_position_by_fft(signal=raw, threshold_freq=frc, frequency_amplitude=fra):
				fft_of_signal = np.fft.fft(signal)
				outlier = np.max(signal) if abs(np.max(signal)) > abs(np.min(signal)) else np.min(signal)
				if np.any(np.abs(fft_of_signal[threshold_freq:]) > frequency_amplitude):
					index_of_outlier = np.where(signal == outlier)
					return index_of_outlier[0]  
				else:
					return None

 			y_with_outlier = np.asarray( raw )#hr1
			outlier_positions = []
			for ii in range(10, y_with_outlier.size, 5):
				outlier_position = detect_outlier_position_by_fft(y_with_outlier[ii-5:ii+5])
				if outlier_position is not None:
					outlier_positions.append(ii + outlier_position[0] - 5)
			outlier_positions = list(set(outlier_positions))
			opindex.append(outlier_positions)
			sum_outlier = len( outlier_positions )
			op.append(sum_outlier)
			oraw.append(raw)

		#dicts
		out_dict = dict(izip( vitals,op) )
		out_dict_index = dict(izip( vitals, opindex) ) #outlier_positions) )
		rawdata = dict(izip( vitals,oraw) )
		stats = dict(izip( vitals,ostats) )
	
		#series dataframe
		#d = dict(outlier = 
		#cl = pd.Series(	

		#closure call for counts
		totalcounts = fftc(out_dict)   #counts for all vitals from closure
		return totalcounts, out_dict_index, rawdata,stats

	totalcounts,indx,raw,stats = total()
	vtl = 'hr1'
	print 'TOTALS ', totalcounts[vtl]
	outlier_positions, y_with_outlier, (mu,std) = map(lambda x: x[vtl], [indx, raw, stats] )
	numo = len(outlier_positions)	

	#fft plot 
	COLOR_PALETTE = ["#348ABD","#A60628","#7A68A6","#467821","#CF4457",	"#188487", "#E24A33"]
	plt.figure(figsize=(12, 6));
	plt.scatter(range(y_with_outlier.size), y_with_outlier, c=COLOR_PALETTE[0], label='Original Signal');
	plt.scatter(outlier_positions, y_with_outlier[np.asanyarray(outlier_positions)], c=COLOR_PALETTE[-1], label='Outliers');
	plt.legend();
	#plt.title('HR_FFT outlier detection', fontsize=17)
	plt.title('FFT-%s %d:%d  $\mu = %0.0f ,\/\ \sigma = %0.0f$ \n threshold= %.2f , amp= %.2f' % (vtl,numo,pltsize,mu,std,frc,fra), fontsize=17)
	plt.show()
	
#	def fft_vector(x):	
#		#rolling_window or rolling_apply
#		frc,fra,pltsize = 0.1, 0.01, 1000	
#		threshold_freq=frc, frequency_amplitude=fra
#		fft_of_signal = np.fft.fft(x)
#		outlier = x.max if abs(x.max) > abs( x.min ) else signal.min
#		if (np.abs(fft_of_signal[threshold_freq:]) > frequency_amplitude):
#			#index_of_outlier = np.where(x == outlier)
#			#return index_of_outlier[0]  
#			return 1
#		else:
#			return 0
#
#		for ii in range(10, y_with_outlier.size, 5):
#			outlier_position = detect_outlier_position_by_fft(y_with_outlier[ii-5:ii+5])
#			#if outlier_position is not None:
#			#	outlier_positions.append(ii + outlier_position[0] - 5)
#			#outlier_positions = list(set(outlier_positions))
#
#	fft_v = lambda x: fft_vector(x)	
#	thmi['outlier'] = rolling_apply( dt[i],5,fft_v )
#	thmi['algo'] = thmi.apply(lambda x: 'fft' )


# -- bcp -- #
import offline_changepoint_detection as offcd
from functools import partial
def bayeschangepoint(dth, dtm):
	def cleandata(dth):
		dth = dth.drop(['gender'],1)
		th = dth.copy()  
		th2 = th.reset_index(drop=True).drop_duplicates(['source','subject_id','realtime','variable']) 
		th2 = th2.set_index(['source','subject_id','realtime','variable'])
		th2 = th2.unstack()
		th2.columns = th2.columns.droplevel(0)
		th2 = th2.reset_index()
		mu = th2.hr1.mean(); 
		std = th2.hr1.std() 
		mmax = mu+std; mmin= mu-std;

		#grouby by subject_id
		g = th2.groupby( ['subject_id'] )
		sidgroup = [ v for k,v in g ]
		signal = []
		r=[] #0=raw #1=norm
		for i,s in enumerate(sidgroup):
			sid1     = []
			raw = np.fromiter(chain.from_iterable( s[['hr1']].values ), dtype=float )
			sid1.append( raw )
			
			mu  = s[['hr1']].mean() 
			std = s[['hr1']].std() 
			mmin = mu-std; mmax = mu+std

			sid1.append( mu )
			sid1.append( std )
			def nf(x):
				if x>mmin & x<mmax:
					return x	
			flt = s[(s[['hr1']] > mmin) & (s[['hr1']] < mmax) ]
			norm = flt[ pd.notnull( flt['hr1'])  ]['hr1'].values
			#norm = s.where( s>0, s['hr1'], axis='index')
			sid1.append( norm )
			r.append( sid1 )
			#0=raw, 1=mu, 2=std, 3=norm
		return r

	def cleanmimic( dtm ):
		#long data just select
		mc = dtm.copy()
		hr = mc[ mc['variable']=='hr2']
		#group by subject_id
		hr_group = hr.groupby('subject_id')
		r = [] 
		for i,s in hr_group:
			tmp =[]
			mu = s.value.mean()
			std = s.value.std()
			mx = mu+std
			mn = mu-std
			v = s.value.values
			norm = [ vv for vv in v if vv>mn and vv<mx ]
			tmp.append(v)
			tmp.append(mu)
			tmp.append(std)
			tmp.append(norm)
			r.append(tmp)
		#0=raw 1=mu 2=std 3=norm
		return r

	s   = cleandata(dth)
	s1  = cleanmimic(dtm)
	subject = 15

	allt = dth.drop(['gender'],1)
	th = allt.copy()  
	th2 = th.reset_index(drop=True).drop_duplicates(['source','subject_id','realtime','variable']) 
	th2 = th2.set_index(['source','subject_id','realtime','variable'])
	th2 = th2.unstack()
	th2.columns = th2.columns.droplevel(0)
	th2 = th2.reset_index()
	raw = th2.hr1.values
	print 'raw ', len(raw), raw[:1000]
	raw = raw[:100]

	raw = np.concatenate( (s[subject][0], s1[subject][0]), axis=0  )
	#norm= np.concatenate( (s[subject][3], s1[subject][3]) )

	data = raw
	Q, P, Pcp = offcd.offline_changepoint_detection(data, partial(offcd.const_prior, l=(len(data)+1)), offcd.gaussian_obs_log_likelihood)
	fig, ax = plt.subplots(figsize=[18, 16])
	ax = fig.add_subplot(2, 1, 1)
	ax.plot(data[:])
	mu = s[0][1]; std= s[0][2]
	plt.title(r'$HR Bayes change-point : \/\ \mu = %0.0f ,\/\mu \/\ \sigma = %0.0f $' % (mu,std), fontsize=17)
	ax = fig.add_subplot(2, 1, 2, sharex=ax)
	ax.plot(np.exp(Pcp).sum(0))
	#plt.title('HR Bayes change-point', fontsize=17)
	plt.show()

# -- kernreg -- #
def kernreg(x):
	'''kernelregression http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gp_regression.html#example-gaussian-process-plot-gp-regression-py
	'''
	pass

#---vectorized alert counts 1-DIM---#
def fft_vector(x):	
	'''-- rolling_window or rolling_apply
		-- return 1 value at time
	'''
	try:
		frc,fra = 0.1, 0.01
		threshold_freq=frc; 
		frequency_amplitude=fra
		
		fft_of_signal = np.fft.fft(x)
		outlier = x.max() if abs(x.max()) > abs(x.min()) else x.min()
		if np.any(np.abs(fft_of_signal[threshold_freq:]) > frequency_amplitude):
			arr = np.where(x == outlier,1,0)
			return arr[0]
		else:
			return 0
	except:
		return 0
## stat-metrics: expected no. of alerts in period -ith alert

def bayes_cp_vector(x):
	''' OFFLINE: get log-probability, Pcp, take exp. sum get p(t_i, is_changepoint)
		input:: 
			1. prior of successive[a=cp,b=cp] at t_distance
			2. likelihood_data:[s_sequence, t_distance] no changepoint 
		ONLINE: gives prob_distribution(mass) of P(t) not_cp in [1,2,...n]; n=0 is P(t) is changepoint
	'''
	# rolling_apply -> lambda x ;
	try:
		Q, P, Pcp = offcd.offline_changepoint_detection(x, partial(offcd.const_prior, l=(len(x)+1)),\
		   	offcd.gaussian_obs_log_likelihood)
		#print '** log-prob change-point t_i ',len(Pcp), '\n', Pcp
		cp = np.exp(Pcp).sum(0)
		#print 'cp\n ', cp
		return cp[0]
	except:
		return 0

#log transform
#https://pythonhosted.org/PyQt-Fit/KDE_tut.html#transformations

import pyqt_fit.bootstrap as bs
import pyqt_fit.kernel_smoothing as smooth
cimaxmin=[[]]
global cimaxmin
cimaxmin.append([])

##def kern_vector(xx ): 
##	'''http://statsmodels.sourceforge.net/devel/generated/statsmodels.nonparametric.kernel_regression.KernelReg.html#statsmodels.nonparametric.kernel_regression.KernelReg
##	http://nbviewer.ipython.org/github/carljv/Will_it_Python/blob/master/MLFH/CH2/ch2.ipynb
##*** https://pythonhosted.org/PyQt-Fit/NonParam_tut.html ***
##	http://sfb649.wiwi.hu-berlin.de/fedc_homepage/xplore/tutorials/xlghtmlnode34.html
##	http://comments.gmane.org/gmane.comp.python.pystatsmodels/10720
##	http://comments.gmane.org/gmane.comp.python.pystatsmodels/12442
##***	http://statsmodels-np.blogspot.in/
##	http://www.mathworks.com/help/stats/nlpredci.html
##***	http://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/
##	http://stackoverflow.com/questions/16707141/python-estimating-regression-parameter-confidence-intervals-with-scikits-boots
##	http://www.statmethods.net/advstats/bootstrapping.html
##	https://github.com/cgevans/scikits-bootstrap/blob/master/scikits/bootstrap/bootstrap.py
##	http://www.r-bloggers.com/a-kernel-density-approach-to-outlier-detection/
##	'''
##
##	''' stats.model -> multivariate -> cdf/pdf
##		pyqtfit -> bootstrap/grid -> ci
##		but what does multivariate even mean ?? ie average of values... i think the scikits-bootstrap will just take a bunch of array, take some average and bootstrap over that..
##		multivariate stat loses interpretability
##	'''	
##	
##	'''
##		univariate: just follow example
##	 '''
##
##	#!!! **WITH WINDOW
##	##--column values/not group object
##	x = xx
##	#x = xx['value'].values
##	#x = xx.values
##	#x.astype(float)
##	ws = len(xx)
##	print 'X', x[:10], len(x),ws
##
##	#--grid/index
##	#add/mult is element-wise with scalar.  
##	#pass imaginary number to np.r_ return (0:ws) inclusive, by skipping last
##	imgn = np.complex(ws)
##	grid = np.r_[0:ws:imgn] 
##	xindex = np.asarray( range( ws ) )
##	print 'imaginary', imgn
##	print 'grid ', grid
##	print 'xindiex', xindex
##	print 'grid lens', len(grid), len(xindex)
##
##	result = bs.bootstrap(smooth.LocalPolynomialKernel1D,\
##		   	xindex, x, eval_points = grid, fit_kwrds = {'q': 2}, CI = (95,99))
##	rmax = result.CIs[0][0,1]
##	rmin = result.CIs[0][0,0]
##	#print 'result ', result.shape(), result[:10]
##	print 'CI ', result.CIs 
##	print 'result', result
##	print 'max--min--xo', rmax, rmin, x[0]
##	print 'len ',len(x),len(rmax), len(rmin)
##	global cimaxmin
##	cimaxmin[0].append(rmax[0])
##	global cimaxmin
##	cimaxmin[1].append(rmin[0])
##
##	#sns.tsplot(g, time="realtime_x", condition="krn", value="value");
##	#exit(0)
##		
##	plt.figure()
##	plt.plot(grid, result.y_fit(grid), 'r', label="Fitted curve")
##	plt.plot(grid, result.CIs[0][0,0], 'g--', label='95% CI')
##	plt.plot(grid, result.CIs[0][0,1], 'g--')
##	plt.plot(xindex, x[:], 'o', label='Data')
##	plt.fill_between(grid, result.CIs[0][0,0], result.CIs[0][0,1], color='g', alpha=0.25)
##	plt.legend(loc='best')
##	#plt.show()
##	#exit(0)
##	#take max of rolling_window.. 
##	##mmx = max(rmax) ; mmn = min(rmin)
##	#outlier = np.where( xx>rmax[0] & xx<rmin[0] ) #, axis='index')
##	outlier =  1 if x[0]>rmax[0] or x[0]<rmin[0] else 0 
##	print 'outlier ', [ outlier,rmax[0],rmin[0] ]
##	return outlier
##
	#return (outlier,rmax[0],rmin[0])
	#arr = np.array([ [outlier, rmax[0], rmin[0]] ] ) #, dtype=np.float64)
	#arr = np.array( [outlier, rmax[0], rmin[0]], dtype=np.float64)
#	return pd.Series({'krn': float( outlier), 'krncimax':float(rmax[0]),'krncimin':float( rmin[0]) })
	#return arr
	#return pd.DataFrame(np.array( [[outlier, rmax[0], rmin[0]] ]), columns=['krn','krncimax','krncimin'])
	#return np.array( [outlier,rmax[0],rmin[0] ] )
	#return [outlier,rmax[0],rmin[0] ]
	#return a group
	#x['krn'] = outlier
	#return x


def kern_vector2(xx ): 
	''' * NOT WINDOW-ing ; apply()
	'''
	# -- grouped object, passed in with lambda
	#xv = x[ x.iloc[['value']] ]
	x = xx['value'].values
	print 'x', x[:4], len(x)	
	
	#--grid/index
	#add/mult is element-wise with scalar.  
	#pass imaginary number to np.r_ return (0:ws) inclusive, by skipping last
	imgn = np.complex(len(x))
	grid = np.r_[0:len(x):imgn] 
	xindex = np.asarray( range( len(x) ) )
	#print 'grid lens', len(grid), len(xindex)

	#--bootstrap
	try:
		result = bs.bootstrap(smooth.LocalPolynomialKernel1D,\
		   	xindex, x, eval_points = grid, fit_kwrds = {'q': 2}, CI = (95,99))
		rmax = result.CIs[0][0,1]
		rmin = result.CIs[0][0,0]
		#print 'minmax', len(rmax), len(rmin)
		
		#--boolean array outlier
		#--vectorized lambda passes each element to expression 
		v = xx['value'].values
		c = np.where( v>rmax, 1,
				np.where( v<rmin,1,0))
		xx['krn'] = c 
		#at = ['krn'] * len(x)
		#xx['alert_type'] = at
		print 'count\n', xx['krn'].value_counts()
		#pri('xx',xx.head())
		return xx
	except Exception:
		z = np.zeros(len(x) )
		xx['krn'] = z
		return xx
#########plot############################	
#	# -- plot
#	sns.set_context('paper')
#	pal = sns.color_palette("Set1", 4)
#	print 'colors', pal
#	fig, ax = plt.subplots()
#
#	clnc = xx.copy()
#	clnc['krnlbl'] = clnc['krn'].map(lambda x: x==0 and 'signal' or x==1 and 'outlier' )
#	c = range(len(xx) )
#	clnc['rr'] =c
#	#plt.figure()
#	#sns.lmplot("rr", "value", clnc, hue="krnlbl", palette="Set1", ax=ax, lowess=True, line_kws={"color": ".2"});#fit_reg=False, ci=95,ax=ax);
#	sns.lmplot("rr", "value", clnc, hue="krnlbl", palette="Set1", ax=ax,fit_reg=False, ci=95); 
#	#sns.tsplot(clndt, time="realtime_x", condition="krn", value="value");
#	ax.plot(grid, result.y_fit(grid), '-', color='.2', label="Fitted curve", linewidth=1)
#	ax.plot(grid, result.CIs[0][0,0], '--',color='.2', label='95% CI')
#	ax.plot(grid, result.CIs[0][0,1], '--', color='.2' )
#	#plt.plot(xindex, x[:], 'o', label='Data')
#	ax.fill_between(grid, result.CIs[0][0,0], result.CIs[0][0,1], color='grey', alpha=0.25)
#	ax.legend()#.draw_frame()
#	ax.set_xlim(0,400 )
#	o = len( xx[xx['krn']==1])
#	n = len(xx.index); print'n',n,o
#	ax.set_title(r'kernel regression dialysis : readings= %d , alerts=%d' % (n,o), fontsize=17)
#	ax.set_xlim(0,100 )
#	plt.show()
########################################
	#return group







	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
## alert visualization using euclidean distances ##
#http://nbviewer.ipython.org/github/carljv/Will_it_Python/blob/master/MLFH/ch9/ch9.ipynb

## plot of hyperplane on scatterplot
## http://nbviewer.ipython.org/github/sujitpal/statlearning-notebooks/blob/master/src/chapter9.ipynb

# -- model -- #
# lasso regression of rank for patient based on alert-term fr
# http://nbviewer.ipython.org/github/carljv/Will_it_Python/blob/master/MLFH/ch6/ch6.ipynb
def countplotdataframe(dth, dtm):
	algos = ['fft','bayes','kern','pm']
	vital =['sys','dia','hr1','ox','hr2','wht']
	counts = []
	#get outlier index, create boolean index with that
	#telehealth: wide, so each vital own column
	#mimic2: long so vitals column	

	
def countsrugplot():
	pass

#--- count up alerts --#
def getcnt(a):
	return a
def Count():
	pp = pprint.PrettyPrinter(depth=6)
	a = dict(sys=0,dia=0,hr1=0,ox=0,hr2=0,wht=0)
	itr = [0]
	def C(a=a,itr=itr):
		'''closure to sum to a
		'''
		def b( cnt ):
			for k in cnt:
				at = a.get(k,0)
				bt =  cnt.get(k,0) 
				if not isinstance(bt, int):
					print '****bt not int **** ', k, sum(itr), len( bt ), type( bt ), #bt
				#print cnt[k]
				if not isinstance(cnt[k], int):
					print '****cnt not int **** ', k, sum(itr), len( cnt[k] ), type(cnt[k]) #, cnt[k]
				if not isinstance(a[k], int):
					print '****a not int **** ', k, sum(itr), len( a[k] ), type(a[k])
				print '*** a *** ',sum(itr),' ', a[k]
				print 'a ', a
				a[k] = a.get(k,0) + cnt.get(k,0) 

			itr.append(1)
			#print '--new call', len(a)
			#pp.pprint(a)
			return a
		#lambda a: getcnt(a)
		return b
	c=C()
	return c

fc=Count()
t = getcnt(fc)
#gc = lambda x,fc:
print '!!**t ** ', t
def toyalgo(fc):
	vitals = ['sys', 'dia', 'hr1', 'ox', 'hr2', 'wht']
	cc = np.random.randint(0,20,size=6)
	cnt = dict(izip( vitals, cc) )
	abc = fc(cnt)
	return abc
print '****&&& fc ****&&&'
toyalgo(fc)
toyalgo(fc)
toyalgo(fc)
abc = toyalgo(fc)
print 'abc', abc
#print 'ttt', map(lambda x:t(x), t)


def time3d(data):
	fig = plt.figure()
	ax = fig.add_subplot(111,projection='3d')
	#ax = Axes3D(fig)
	
	xs = np.arange( data.shape[0] )
	ax.bar(xs, data.dia, zs=0, zdir='y', color='y', alpha=0.5)
	ax.bar(xs, data.sys, zs=10, zdir='y', color='r', alpha=0.5)
	ax.bar(xs, data.hr1, zs=20, zdir='y', color='g', alpha=0.5)
	#ax.bar(xs, data.ox, zs=30, zdir='y', color='g', alpha=0.5)
	ax.bar(xs, data.hr2, zs=30, zdir='y', color='b', alpha=0.5)
	ax.bar(xs, data.wht, zs=40, zdir='y', color='m', alpha=0.5)


	#ax.set(zlim=(0,300))
	ax.set_xlabel('X')
	ax.set_ylabel(['dia','sys','hr1','hr2','wht'])
	ax.set_zlabel('Z')
	plt.show()




#----------------
#--- boosting --#
#balance dataset
#cost-sensitive learning p147 MLinAction



#----------------
#--- model --#

# - _coefficient plot

# - _effect size 

# - _monty hall


#data/otter_bayes.py
#----------------
def pri(msg, dt):
   output = StringIO()
   dt.to_csv(output)
   output.seek(0)
   pt = prettytable.from_csv(output)
   print '\n**',msg,'\n',pt

def main():
# -- format data for fnc -- #
	#format
	# -- gender
	th_data['gender'] = th_data['gender'].apply(lambda x:x==' m' and str('Male') or str('Female') )
	mc_data['gender'] = mc_data['sex'].apply(lambda x:x=='M' and str('Male') or str('Female') )

	# -- source
	th_data['source'] = th_data['subject_id'].map(lambda x: 'telehealth') 
	mc_data['source'] = mc_data['subject_id'].map(lambda x: 'mimic') 


	#melt to long-form
	thm = th_data.reset_index(drop=False) #dont drop realtime
	thd = thm.drop(['dt2','dt3'],axis=1 ) #,inplace=True)
	thmelt = pd.melt(thd, id_vars=['subject_id','gender','source','realtime']) #,var_name=['itemid'] )
	thmelt['value'].map(lambda x : np.log(x) )
	#pri('th long', thmelt.head())

	#mimic(long default) , merge
	mcm = mc_data.reset_index(drop=True)
	mc_data['variable']=mc_data['itemid'].map(lambda x:x)
	mc_data['value']=mc_data['value1num'].map(lambda x:x)
	mcd = mc_data.drop(['itemid','value1num','value2num','index','charttime','sex','hospital_expire_flg','description','value1uom','value2uom'],axis=1 )
	#pri('mimic dropped', mcd.tail() )
	mcd.reset_index(inplace=True)
	#print 'mcd type &&&& ', mcd['realtime'].apply(type) 

	mcd = mcd.dropna()
	#print 'mcd type &&&& ', mcd['realtime'].apply(type) 
	if any( pd.isnull( mcd) ):
		print 'mcd dropped not null'
	#merge
	thmi = pd.merge(thmelt,mcd,on=['subject_id','gender','source','variable','value'], how='outer' )
	pri('merged data set', thmi.head() )
	pri('merged mimic check', thmi[thmi['source']=='mimic'][:5])
	#print '*****len-check****', len(thmi[ thmi['source']=='mimic'] ), len(mcd), len(mcm), len(mc_data)

## data format section  above
#########################################################################

## ** using thmi for everything ** ##


# -- summary -- 
	def mainsummary():
		#http://stackoverflow.com/questions/22248580/how-to-summarise-data-over-several-years-into-one-dataframe?rq=1
		#average days of sample
		#timeplotH(th_data, title='telehealth')
		#timeplotH(mc_data, title='mimic2')

		# -- frequency vs %change_HR
		##census_th(thmelt)
		##census_mmc(mcd)

		# -- final week %change diff_HR
		#tpmf = census_pmf( thmelt,mcd ) #thmi
		#mpmf = census_mimic_pmf(mcd)
		#histopmf( tpmf, mpmf )

		# -- boxplot
		census_box(thmi)

		#fit lognormal distribution
		#distributionsFG(thmi,row='source',col='variable', val='value')


# -- alerts --
			# vectorize
		#http://stackoverflow.com/questions/15487022/customizing-rolling-apply-function-in-python-pandas
		#http://stackoverflow.com/questions/24032282/create-contour-plot-from-pandas-groupby-dataframe
		#vitals = ['sys'] #, 'dia', 'hr1',  'ox', 'hr2', 'wht']
		#http://stackoverflow.com/questions/23862429/using-apply-in-pandas-externally-defined-function
		#http://nbviewer.ipython.org/gist/nipunreddevil/6947228
#	if(1):
#		if(0):	
#			#gender	 
#			gender = th_data[th_data['gender']==' m']	
#			males = gender[['wht','subject_id']];	
#			maleid19 = males[males['subject_id']==19]
#			m2 = maleid19.copy()
#			logfitcdf(males)
#			exit(0)
#
#			#logboostci(m2)
#			#test(malesgrp)
#			test(smple)
#			exit(0)
			# confidence interval
			#dtb = boostci(th_data) 
			#dtv = dtb[dtb['vitals']=='sys']
			#print 'dtv check', dtv.tail(), dtv.describe()

			# fft
			#fft(thmelt,mcd  )
			
			# bayes change point
			#bayeschangepoint(thmelt, mcd)

	def mainhdf5(df=thmi, hdf5='store2.h5' ):
		#http://stackoverflow.com/questions/16997048/how-does-one-append-large-amounts-of-data-to-a-pandas-hdfstore-and-get-a-natural?lq=1
		#http://stackoverflow.com/questions/15939603/append-new-columns-to-hdfstore-with-pandas?rq=1
		#http://pandas.pydata.org/pandas-docs/dev/io.html#multiple-table-queries
#http://mldata.org/about/

		''' multiple table: 
					selector table with indexed_col, that other tables indices match 
					query over selector, quick, but get lots of data back
					it is like having a wide table, but more efficient
			   /refs/pandas.pdf ch18 hdf5 table examples, query
		'''
		'''**  hdf5 not reclaim space, delete/add just expands
			   time_format not safe, NaT convert to int64 etc
		'''
		'''row append only, not column... therefore make long table and append to it
			df['alert_type'] df['alert_value']
		'''
		pd.set_option("io.hdf.default_format","table")
		hdf5 = drcty2 + hdf5 
		#hdf5 = drcty2 + 'store.h5' 

		store = pd.HDFStore(hdf5) 

		store.append('thmi',df)

		print "** HDFStore type ** ", store.root.bycp._v_attrs.pandas_type
		print 'num rows= ', store.get_storer("bycp").nrows

			#load hdf5 file
			#with pd.get_store(hdf5) as store:
			#	print "\n** hdfs type ** \n", store.root.bycp._v_attrs.pandas_type
			#	print 'num rows= ', store.get_storer("bycp").nrows
			#	th = store.select('bycp')
			#	pri('bayes_cp alerts', th.head() )
			#	print th.bycp[:20]
			#	#th = store['bycp']  # load it

	def maincsv(dt=None, csvfile='thmi_alerts.csv'):
		'''write all the alerts to csv
		   thmi gets ['fft'] ['krn'] ['bcp']
		   stack to ['alert_type'] ['alert_value']
		'''
		drcty2 = '/home/solver/project/data/'
		csvf = drcty2 + csvfile

		if not os.path.exists( csvf ) :
			print '\n:::you are writing csv file ',csvfile,':::\n'
			dt.to_csv(csvf, sep='\t', encoding='utf-8')

		if os.path.exists( csvf ):
			print '\n',csvfile,' exists, not overwritten\n '
			nf = csvf[:-4] + '_TEMP_.csv'
			print '\n',nf,' written instead\n '
			dt.to_csv(nf, sep='\t', encoding='utf-8')
		
	def mainfft(dt=thmi):
		print "\n::: performing fft detection :::\n"
		''' grouped only by ['variable'] '''
		fft_v = lambda x: fft_vector(x)	
		#pri('fft bfre alert', dt.head() )
		#dt['fft'] = dt.index.map(lambda x:-1)
		tg = dt.groupby(['variable'])
		gg = []
		for k,g in tg:
			# return 1 val at time
			g['fft'] = pd.rolling_apply( g['value'], 10, fft_v )
			gg.append(g)
			print k, g[['fft']][:15]
		th =pd.concat(gg)
		#pri('checking dt fft', dt.head(25))
		print 'fft alerts\n', th['fft'].value_counts()
		return th
		#pri('th', th.head(15) )
		#print 'th', th[['fft','variable']][:100]
	
	def mainkernreg(dt=thmi):
		print "\n::: performing kernel regression detection _apply :::\n"
		'''grouped by variable, subject_id 
		#http://stackoverflow.com/questions/24272398/python-cleaning-dates-for-conversion-to-year-only-in-pandas
		http://stackoverflow.com/questions/9155478/how-to-try-except-an-illegal-matrix-operation-due-to-singularity-in-numpy?rq=1
		'''
		# -- munge
		#dt['krn'] = dt.index.map(lambda x:-1)

		# -- datetime64 issue
		pd.to_datetime( dt['realtime_x'], coerce=True )
		#dt['rt']=dt[['realtime_x']].apply(lambda x: np.datetime64(x).astype(dtt.datetime)) 
		## clnc['rt'] = pd.to_datetime(clnc['realtime_x'])
		#df = dt.ix[ clnc ]

		##cln = dt[ (dt['subject_id']==1) &( dt['variable']=='dia') ]

		# -- vectorized, but pass each group with lambda
		kr_v = lambda x: kern_vector2(x )
		cln = dt.groupby(['subject_id','variable'],as_index=False,group_keys=False).apply(kr_v)
		#pri('krn', cln.head() )
		print 'krn alerts\n', cln['krn'].value_counts()
		return cln

			  	#  numpy.linalg.linalg.LinAlgError as err:
				#   if 'Singular matrix' in err.message:
				#	      # your error handling block
				#		    else:
			  	#				    raise


		# -- plot
		#cln['krnlbl'] = cln['krn'].map(lambda x: x==0 and 'signal' or x==1 and 'outlier' )
		#sns.set_context('paper')
		#clnc = cln.copy()
		#c = range(len(cln) )
		#clnc['rr'] =c
		##clnc.set_index('rt', inplace=True)
		#pri('plot', clnc.head() )
		##clndt['value'].plot()
		#sns.lmplot("rr", "value", clnc, hue="krnlbl", palette="Set1", fit_reg=True, ci=95);
		##sns.tsplot(clndt, time="realtime_x", condition="krn", value="value");
		#plt.show()

		#exit(0)
		#clnu = cln.apply( lambda x: x.unstack() )
		#clnd = clnu.reindex( cln.index.get_level_values(0).unique() )
		#clnd = clnd.apply(kern_vector2)
		
		#iteration flattens multiple group pairs
	##	gg = []
	##	for (k1, k2), g in cln.groupby(['subject_id','variable'], axis=0):
	##		#gg = np.vectorize(kern_vector)(g['value'].values, wsc 	) 
	##		#g['krn'] = g['value'].apply( kern_vector )
	##		#g['krn'] = g[['value']].apply( kr_v )
	##		#g['krn'] = g.apply( lambda x: kern_vector2( x[ x[['value']].iloc() ], lenx=ll ), axis=0 )
	##		print('keys ', k1,k2)
	##		ll = g[['value']].count()
	##		print('g ', g[:3] )
	##		print '### ll ', ll
	##		# -- pass each group to kr_v lambda
	##		g.apply( kr_v )
	##		print 'group vals** ', k1,k2, g['value'][:10]
	##		print k1,k2, g[['krn']][:50]

	##		sns.tsplot(w, time="realtime_x", condition="krn", value="value");
	##		gg.append( g )
	##		exit(0)

	##	th = pd.concat(gg)
	##	pri('kernel_regression',th.head() )

		
	if(0):
		print "\n::: performing kernel regression detection _window=10 :::\n"
		window_size = 50
		kr_v = lambda x: kern_vector(x )#, window_size)		
		pri('thmi', thmi.head() )	
		thmi['krn'] = thmi.index.map(lambda x:-1)
		cln = thmi[ pd.notnull( thmi['value'] ) ]

		gg = []; ww = [] 
		ci = [[]]
		a = []
		grp = thmi.groupby(['subject_id','variable'] ) #.apply(kern_vector)
		for (k1,k2),g in grp:
			print(k1, k2) 
			# -- windows for one group
			g['krn'] =  pd.rolling_apply( g['value'].values, window_size, kr_v )  
			gg.append(g)
			w = pd.concat(gg)

			#plt.figure()
			#sns.lmplot("total_bill", "tip", tips, hue="time", palette="Set1"); #fit_reg=False);
			sns.tsplot(w, time="realtime_x", condition="krn", value="value");
			plt.show()
			exit(0)
			#tv = g[ g['variable']==k2][['value']]
			#outlier_positions = g[ (g['variable']==k2) & (g['krn']) ]
			#COLOR_PALETTE = ["#348ABD","#A60628","#7A68A6","#467821","#CF4457",	"#188487", "#E24A33"]
			plt.figure(figsize=(12, 6));
			plt.scatter(range(len(tv)), tv, c=COLOR_PALETTE[0], label=k2);
			plt.show()
			exit(0)


			plt.scatter(outlier_positions, tv[outlier_positions], c=COLOR_PALETTE[-1], label='outliers');
			plt.legend();
			#plt.title('HR_FFT outlier detection', fontsize=17)
			plt.title('kernel regression outlier detection %s',k2 , fontsize=17)
			plt.show()
			#tv.plot()
			exit(0)
			c = len( cimaxmin[:][0] )
			xlnv = range(c)
			print len(tv.index[10:]), c
			print(k1,k2)
			plt.plot(xlnv, tv[9:], 'o',lw=1, label=k2)
			plt.plot(xlnv, cimaxmin[:][0], 'g--', label='95% CI')
			plt.plot(xlnv, cimaxmin[:][1], 'g--')
			plt.fill_between(xlnv, cimaxmin[:][1], cimaxmin[:][0], color='g', alpha=0.25)
			plt.show()
			ll = g[['value']].count()
			print '### ll ', ll, len( g[['value']] )
			exit(0)

		th = pd.concat(gg)
		pri('kernel_regression',th.head() )

	def mainbayes_changepoint(dt=thmi):
		#if not os.path.exists( hdf5 ): 
		print "\n::: performing bayes-point detection :::\n"


		bp_v = lambda x: bayes_cp_vector(x)
		#dt['bycp'] = dt.index.map(lambda x:-1.0)

		tg = dt.groupby(['variable'])
		gg = []
		for k,g in tg:
			print '##val check ', k,'\n', g.value[:3]
			g['bycp'] = pd.rolling_apply( g['value'], 15, bp_v )
			g['bycp'] = g['bycp'].map(lambda x: x> g['bycp'].quantile(.90) and 1 or 0)
			gg.append(g)

		th = pd.concat(gg)
		pri('bayes changepoint', th.head() )
		return th
	
# -- read write alerts --
# -- write a new file, then change if->(0)	
	#csvf = 'alert100_TEMP_.csv'
	#csvf = 'alert1500mcd.csv'
	csvf = 'alert5000mcd.csv'
	#csvf = 'alert20Kmcd.csv'
	##with new main_bycp function that discretize probs
	#csvf = 'alert1Kmcd_TEMP__TEMP_.csv' 
	if(0):
		fltr = thmi[thmi['source']=='mimic'] 
		smp = fltr
		a=mainfft(dt=smp)
		b=mainkernreg(dt=smp)
		c=mainbayes_changepoint(dt=smp)

		alrt = pd.concat([smp , a['fft'],b['krn'],c['bycp']], axis=1)
		print('thmi concat alerts',alrt.head(50) )
		maincsv(dt=alrt, csvfile=csvf)
	else:
		#print 'in loop'
		f = './data/' + csvf
		hdr =[ 'subject_id',  'gender',  'source',  'realtime_x',  'variable',    'value',   'index',   'timeshift',   'level_0', 'dob' ,'dod', 'realtime_y',  'fft', 'krn', 'bycp']


		alrt_data = pd.read_csv(f, names=hdr,skiprows=1, sep='\t')
		print('loaded alerts from csv', alrt_data.head())



	def maincount_alerts_rug(dt=alrt_data):
		'''row = variable
		   col = alert_ye
		'''
		# -- pre-example
		sns.set_style("whitegrid")
		#d = pd.DataFrame({'row':['a']*9 + ['b']*9 + ['c']*9,
		#					'col': ['fft','krn','bycp']*9,
		#					'val':np.random.randn(27)})
		#print 'data', d

		'''stacking the dataframe
		'''
		# -- merge mimic and telehealth time index
		mt=dt[ dt['source']=='mimic']['timeshift']
		tt=dt[ dt['source']=='telehealth'][ 'realtime_x' ]
		dt['tidx'] = pd.concat( [mt,tt], axis=0)
		dt['timeindex'] = pd.to_datetime(dt['tidx'])
		print type(dt['timeindex']), dt['timeindex'].dtype

		# -- expanding the dataframe wide to long
		# -- melt()
		dcat = pd.concat( [dt,dt,dt], axis=0 )
		dcat['alert_t'] = ['krn']*len(dt['krn']) + ['fft']*len(dt['fft']) + ['bycp']* len(dt['bycp'])   
		dcat['alert_v'] = pd.concat( [dt['krn'] , dt['fft'] , dt['bycp']], axis=0)
			
		dcatt = dcat[['source','subject_id','alert_t','variable','timeindex','alert_v','value']]
		d = dcatt.reset_index(drop=True).set_index(['timeindex'],drop=False ).copy()
		print 'd expand' , d.head(20)

		# -- get the interarrival time
		# -- set the iqt range
		# -- get cumsum over interarrival time
		def deltat(g):
			try:
				g['tavg'] = g[ g['alert_v']==1 ]['timeindex'].diff(1)
				#print g
				return g
			except:
				pass

		def iqt(g):
			try:
				g['iqt'] = g[ g['alert_v']==1 ]['value'].map(lambda x: x > g['value'].quantile(.90) and 1 or x < g['value'].quantile(.10) and 1 or 0)
				#print 'iqt', g
				return g
			except (Exception, StopIteration) as e:
				pass

		def cum(g):
			try:
				#--exact[float64] conversion timedelta to seconds
				#g['tavgsec']= pd.to_timedelta(g['tavg'],unit='d')+pd.to_timedelta(0,unit='s').astype('timedelta64[s]')
				# --exact convert to float64
				g['tavg'] = g['tavg'].fillna(0)
				g['tavgf']= (pd.to_timedelta(g['tavg'],unit='d')+pd.to_timedelta(0,unit='s'))/np.timedelta64(1,'D')

				# --cumsum on filter rows
				g['cumt'] = g[ g['alert_v'] == 1 ]['tavg'].cumsum()
				g['cumt'] = g['cumt'].fillna(0)

				# float64 convert
				g['cumtf'] = (pd.to_timedelta(g['cumt'],unit='d')+pd.to_timedelta(0,unit='s'))/np.timedelta64(1,'D')
				#print 'group type', type(g['cumt'] )
				##print g.head()
				return g

			except (Exception, ZeroDivisionError , StopIteration, ValueError) as e:
				print 'cumulative error\n', e
				pass

		# -- utility
		d.sort_index(axis=0, inplace=True)
		dg = d.groupby(['source','subject_id','alert_t','variable'], as_index=False, group_keys=False)
		#	pd.to_datetime(d['tavg'], format='%H:%M:%S')


		# -- set bycp threshold for probability val to alert
		def quantg(g):
			try:
				# -- vectorized if-else 
				#g['alert_v'] = np.where(g['alert_v']>g['value'].quantile(.75),1,0) 
				g['alert_v'] = np.where(g['alert_v']>.1,1,0) 
				g.drop_duplicates(inplace=True)
				#print 'group', g[:2]
				return g
			except (Exception,StopIteration,TypeError) as e:
				print '**bycp error\n', e
				pass
		
		## duplicate values, therefore have to reset index and drop duplicates for both groups and df original, \
		# or else update does not know which row to update new value to.
		# get_duplicates(), duplicated, drop_duplicates()

		#-- pre-filtered group, 
		#-- post-filter not work over multi-column, does not return unfiltered
		d.reset_index(inplace=True, drop=True)
		db=d[d['alert_t']=='bycp'].groupby(['timeindex','source','subject_id','alert_t','variable'],\
				as_index=True,group_keys=True).apply(lambda x: quantg(x))  #.copy(deep=True) 
		db.drop_duplicates(inplace=True)
		print 'bycp vals', db.head()
		print 'bycp == 1 *** ', db[ db['alert_v']==1][:10], len(db[ db['alert_v']==1])

		#-- update to (unfiltered) data frame
		d1 = d.set_index(['timeindex','source','subject_id','alert_t','variable'],drop=False, inplace=False).copy()
		#print '*** bycp to_update', d[d.alert_t == 'bycp'].head()
		d1.update(db, overwrite=False)
		#print '*** bycp updated', d1[d1.alert_t == 'bycp'].head()
		print '*** bycp updated', d1.head(10)
		d1.reset_index(inplace=True,drop=True)
		#d.set_index(['timeindex'], drop=False, inplace=True)
		#print '*** bycp', d[d.alert_t == 'bycp'].head()

		# -- stupid way to split dataframe and concate alert_t
		dd = d.copy()
		print dd.head()
		#dd.reset_index(inplace=True)
		ddk = dd[dd['alert_t']=='krn']; ddf=dd[dd['alert_t']=='fft']

		d2 = pd.concat( [ddk,ddf,db], axis=0 )
		d2.set_index(['timeindex'], drop=False, inplace=True)
		print 'weird*** ', d2.head(), len(d2) 
		print 'weird*** ', d2.tail(), len(d2) 
		d2.sort_index(axis=0, inplace=True)
		print 'weird*** ', d2.tail(), len(d2) 
		
		#--- bycp-end ---

		# -- get time delta interarrival times of alerts
		d=d2.copy()
		print 'weird d*** ', d.head(), len(d) 
		print 'weird d*** ', d.tail(), len(d) 
		print 'weird d*** ', d.tail(), len(d) 

		d=d.groupby(['source','subject_id','alert_t','variable'],as_index=False,group_keys=False).apply(lambda x: deltat(x) )

		# -- set quartile alerts; to get FP,FN
		d=d.groupby(['source','subject_id','alert_t','variable'],as_index=False,group_keys=False).apply(lambda x: iqt(x) ) 

		# -- get cumulative time
		print '*** bycp pre 2', d[d.alert_t == 'bycp'].head()
		d = d.groupby(['source','subject_id','alert_t','variable'],as_index=False,group_keys=False).apply(lambda x: cum(x) ) 
		print '*** bycp2', d[d.alert_t == 'bycp'].head()
		#print d.describe() #print d.head()

		# -- filter out timedeltas eq 0 
		aa=pd.to_timedelta('00:00:00')
		print '*** bycp tod2', d[d.alert_t == 'bycp'].head()
		d = d[ pd.to_datetime( d['cumt'] ) - pd.to_timedelta('00:00:00') > pd.to_timedelta('00:00:00') ]
		print '*** bycp tod++', d[d.alert_t == 'bycp'].head()

		# fp/fn vs iqt boxplot
		# -- 1 box per column -> use pivottable
		def fp(x):
			iqt = x['iqt']
			alv = x['alert_v']
			x['fpfn'] = x.apply(lambda x: x['iqt']==0 and x['alert_v']==0 and 'TN'
										or x['iqt']==1 and x['alert_v']==1 and 'TP' 
										or x['iqt']==0 and x['alert_v']==1 and 'FP'
										or x['iqt']==1 and x['alert_v']==0 and 'FN',
										axis=1 )
			return x

		d = d.groupby(['source','subject_id','alert_t','variable'],as_index=False,group_keys=False).apply(lambda g: fp(g) )
		print '*** bycp3', d[d.alert_t == 'bycp'].head()
		#print 'd len', len(d['subject_id'].unique() ), d['subject_id']
		print dcatt.info()
		pri('catt', dcatt.head() )
		print 'sid unq', len( dcatt['subject_id'].unique() )
		#c= c.unstack()
		#pri('cmplte', c.head() )
		c = d.copy()
		print '*** bycp4', c[c.alert_t == 'bycp'].head()
		c.reset_index(inplace=True,drop=True)
		cp = c.pivot_table(rows=['subject_id','alert_t'],
						   cols=['fpfn'],
						   values=['alert_v'],
						   aggfunc=lambda x: x.count() ) 
		print ('cp alert type', c['alert_t'].unique() )
		print('cp',cp.head(15) )
		print cp.info()
		#b = sns.FacetGrid(d, col='alert_t',palette="husl",margin_titles=True)
		#b.map(sns.boxplot,c)
		sns.boxplot(cp)


		# -- rugplot
		#(c1, c2, c3, c4, c5 ) = sns.color_palette("husl", 6)[:5]
		g = sns.FacetGrid(d, col='alert_t', row='variable',size=1, aspect=3, palette="husl",margin_titles=True)
		g.map(sns.rugplot,'tavgf') #,height=.5)
		print sns.axes_style()
		sns.despine(left='False')
		g.fig.subplots_adjust(wspace=1.2, hspace=.3);
		g.set_axis_labels(['time diff']);
		g.set(yticks = [])

		# -- cdf plot
		# http://nbviewer.ipython.org/github/nicolasfauchereau/NIWA_Python_seminars/blob/master/4_Statistical_modelling.ipynb
		#pd.tslib.repr_timedelta64(np.timedelta64(180487000000000,'ns'))
		print 'tavgf ## ', type(d['tavgf']) , d['tavgf'].dtype
		print('##@@d',d.head() )
		c = sns.FacetGrid(d, col="alert_t")	
		c.map(  sns.distplot, 
				"cumtf", 
				kde=True, 
				kde_kws={'cumulative':'True'},
				fit=stats.expon )
		#loop subplots over alert_t

		#c.map( sns.kdeplot, "cumtf", cumulative=True )
		c.set_axis_labels(['time diff']);

		# - kde plot of freq(1/a, 1/b, 1/c) <- cdf
		# http://stackoverflow.com/questions/6298105/precision-of-cdf-in-scipy-stats
		# -- get slope
		a1 = c.facet_axis(0,0)
		#print 'a1 ', a1.get_children()
		mx = a1.get_children()[2]._x
		my = a1.get_children()[2]._y	
		#print 'm\n', mx, my
		maxm =0
		minm =1
		slopes = [(x,y) for x,y in zip(mx,my) if y/x-0.2<0 or y/x+0.2>0 ]
		#print 'slopes ', slopes[:2]

		# box plots
		# -- binned frequency
		# max time
		# avg_time
		at = d.copy()
		#boxat = at.pivot_table(rows=['subject_id','alert_t'],
		#				   	values=['tavg'],
		#				   	aggfunc=lambda x: x.mean() ) 
		#pri('boxplot avgT', boxat.head() )

		# diff assumptions (Weibull, k=2)
		# violin plot of interarrival times

		# overlap kde for each variable
		# busiest time

		#time by alert-types (top 3) 

		#time by alert-size (top3)

		plt.show()

		# clean up time column
		#http://stackoverflow.com/questions/19350806/how-to-convert-columns-into-one-datetime-column-in-pandas
		#http://stackoverflow.com/questions/17688155/complicated-for-me-reshaping-from-wide-to-long-in-pandas
	'''

		# alert_type 
		#http://nbviewer.ipython.org/github/amplab/datascience-sp14/blob/master/lab4/joins.ipynb
		print('dt',dt.head() )
		s = dt.stack(['bycp']) #['fft','krn','bycp'])
		print('stack',s[:25] )
		#create empty dataframe of size ['values'][alert_type][alert_value]
# 3 * len of dt
		alt['alert_value'] = dt['fft'].map(lambda x: x)
		alt['alert_value'] = dt['krn'].map(lambda x: x)
		alt['alert_value'] = dt['bycp'].map(lambda x: x)
		alt['alert_type'].apply(lambda x: 

		# facet plot
		# rugplots , rows='variable', cols='alert_type'

		# cdf of time to alert

		# 
		# count up the alerts
		# time to alerts
		# rug plot on bottom
		#countalerts(dt)

		#boots(th_data, dtv)
		##pltframe = percent_method2(dtv)
		##ttestboost(dtv)
		#boostpercentplot( pltframe )

		#ci = percent_method(empirical_distribution)
		#toyplotpm(ci)
	'''
	maincount_alerts_rug()





#-boosting-----------------------------------------------------
# score.py  main.py
# weights graph
# wordle of alerts

#-- sampling --#
#reservoir sampling
#mab
#http://blog.yhathq.com/posts/the-beer-bandit.html
#bayes mab (ukrainian seattle data science co.)
#http://cstheory.stackexchange.com/questions/21338/how-much-time-to-recognize-palindromes-in-logarithmic-space

#--- hypothesis model ---#
# ridge regression mlh ch4

#-disease model-- 
#http://blog.yhathq.com/posts/predicting-customer-churn-with-sklearn.html
#hash kernel
#int prg
#percolation
#mixture model
#http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/

#-bayes---------------------------------------------
#http://nbviewer.ipython.org/github/PrincetonPy/Python-Workshop/blob/master/3.Demos.ipynb
#https://probmods.org/occam%27s-razor.html
	

if __name__=="__main__":
	main()




