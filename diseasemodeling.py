
### outline
# clusters {fixed, generative}
# hash, bag-of-words

#http://blog.vene.ro/2011/09/19/dictionary-learning-in-scikit-learn-0-9/
# multi-arm bandit (bayesian optimization)
# network- prob.graph.model, hmm, clique percolation, cssr
# mixed-integer programming
"""

<script type="text/javascript" src="jquery-latest.min.js"></script>
<link href="knowlstyle.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="knowl.js"></script>

"""
### refs
#dirichlet
# http://snippyhollow.github.com/blog/2013/03/10/collapsed-gibbs-sampling-for-dirichlet-process-gaussian-mixture-models/	<br>
# snippyhollow.github.com/blog/2013/03/10/collapsed-gibbs-sampling-for-dirichlet-process-gaussian-mixture-models/	<br>

# gmm
#http://blog.counsyl.com/2013/08/07/detecting-genetic-copy-number-with-gaussian-mixture-models/	<br>
#http://sociograph.blogspot.com/2011/11/clique-percolation-in-few-lines-of.html	<br>

#plots	<br>
#http://blog.revolutionanalytics.com/2009/11/charting-time-series-as-calendar-heat-maps-in-r.html	<br>

#clique-percolation
#http://sociograph.blogspot.com/2013/05/revealing-community-structure-with.html	<br>
#http://mindsknowledge.blogspot.com/2012/02/javascript-tools-for-neuroscience.html	<br>
#http://wwwold.fi.isc.cnr.it/users/thomas.kreuz/research/synchrony.html	<br>
#http://levreyzin.blogspot.com/2012/12/uics-mcs-program-seeking-graduate.html

#logical distance
#http://jeremykun.com/2012/04/21/kolmogorov-complexity-a-primer/

#n-gram
#http://news.rapgenius.com/Atodd-when-harvard-met-sally-n-gram-analysis-of-the-new-york-times-weddings-section-lyrics

#meaningful
#clustering is meaningless : keogh
"""
class object that can classify clusters

"""

from score
from  threshold
class vitals(score):
	def __init__(self):
		
		self.globls = None

### Gaussian mixture model: fixed-clusters 
# reference, 1-alert, 2-alert, 3-alert	<br>
#Choose value on the y-axis which will be alert threshold.  Then use fitted curves to determine at which cycle the sample reaches threshold, and at which cycle the reference threshold reaches that cycle.  Then take difference of sample-reference pair.  Using deltas, normalizes effects of initial conditions and reaction efficiency.   

# delta-reference pairing	<br>
# take difference between ref and sample,(x-axis).  for which value x is equal to set threshold-y.  if y(threshold), what is x given by fit function.  Inverse, get x-value from y, where x is the weights and y is the entropy.  So look at delta of weights, between reference and alert.  Plot that as histogram.

# histo visual inspection of deltas.	<br>
# Can look at histogram and see different clusters.

# assign sample to to a cluster {hard, easy, static, seq-window }	<br>
# Based on reference control data, see that clusters correspond to weight iteration. 


# classification based on assignment	<br>
#The assumption is that each sampled data point come from a distribution over each cluster, which are normally(Gaussian) distributed.  These clusters are hard/easy, correct/incorrrect, and sequential/static.  Each cluster distribution has its own latent, unobserved variables: mean _mu, std deviation _sigma, and weight, where weight represents the amount of total data coming from a particular distribution.  Both the label variables, the label of a sample indicating from which distribution it came, and the unobserved variables must be fitted, using standard statistical method of EM.

# expectation-maximization	<br>
# Initially starts with guess of latent(unobserved) parameters, which is then iteratively improved upon.  During expectation step, use Gaussian parameter estimates to classify each sample.  During maximization step, adjust Gaussian parameter values based on current sample labels.  EM is guarenteed to converge to local max of likelihood, where labels and set of parameters are self-consistent.    



### Chinese Restaurant Process: generative-clusters
#"""
#  Real world data signals can contain many data-types or states, but the observed data is limited, cannot detect all types of state.  Standard clustering like k-means or gaussian mixture assume finite states, even if fed with more data.  
#
#  Nonparametric bayes class of techniques allow some parameters to change with the data.  Instead of fixing the number of clusters to be discovered, allow them to grow generatively with the data.
#
#  An infinite set of latent groups is described by some set of parameters, such as a gaussian with specified mean _mu_i, and standard deviation, _sigma_i.  A group's parameters come from some base distribution (G_0), for each group.  
#  
#  Data is generated by 1.selecting a cluster 2.sampling from that cluster to generate a new point.
#	{for example limited data of 10 friends, asked what they ate yesterday? When deciding alice, bob, etc..n-data chose a group{chinese, pizza}, and then sample from that group to generate a new data point.  How to assign a n-friend to a group?
# 1.restaraunt empty
# 2.aliceperson[0], select a group (table), order food for table(parameters), friends sit at table limited to food of table
# 3.bob person[2], select group-table P_newtable[k=1]: $\alpha/1+\alpha$,  P_oldtable[k=0]: $\1/1+\alpha$
# 4.person[n+1].  P_newtable[k+1]: $\alpha/n+\alpha$, P_oldtable[k]: $n_k/n+\alpha$ where n_k is number of people at table[k]
#
#  Group-table with more people, n_k, more likely to have more people join
#always small probability someone will join new group
#\alpha is a dispersion parameter(prior): the probability of a new group depends on it.  lower \alpha tight clusters, hiher, more clusters in finite points
#
#Table selection similar to dirichlet process.
#"""
# Draw `num_balls` colored balls according to a Polya Urn Model
# with a specified base color distribution and dispersion parameter
# `alpha`.
#
# returns an array of ball colors
def polya_urn_model(base_color_distribution, num_balls, alpha)
  return [] if num_balls <= 0

  balls_in_urn = []
  0.upto(num_balls - 1) do |i|
    if rand < alpha.to_f / (alpha + balls_in_urn.size)
      # Draw a new color, put a ball of this color in the urn.
      new_color = base_color_distribution.call      
      balls_in_urn << new_color
    else
      # Draw a ball from the urn, add another ball of the same color.
      ball = balls_in_urn[rand(balls_in_urn.size)]
      balls_in_urn << ball
    end
  end

  balls_in_urn
end



### polya urn 
"""
#balls = people = data
colors = tables = group
alpha = dispersion parameter

generative: draw ball from urn, return + new ball of same color

#colors satisfy rich-get-richer
similar to dirichlet

# crp vs polya
crp only specifiy table assignment, partition over groups, not group parameters
polya does both, table assignment(colors), group parameters(urn- G_0 distribution)


"""
# Draw `num_balls` colored balls according to a Polya Urn Model
# with a specified base color distribution and dispersion parameter
# `alpha`.
#
# returns an array of ball colors
def polya_urn_model(base_color_distribution, num_balls, alpha)
  return [] if num_balls <= 0

  balls_in_urn = []
  0.upto(num_balls - 1) do |i|
    if rand < alpha.to_f / (alpha + balls_in_urn.size)
      # Draw a new color, put a ball of this color in the urn.
      new_color = base_color_distribution.call      
      balls_in_urn << new_color
    else
      # Draw a ball from the urn, add another ball of the same color.
      ball = balls_in_urn[rand(balls_in_urn.size)]
      balls_in_urn << ball
    end
  end

  balls_in_urn
end



### stick-breaking process
"""
CRP, or polya generate a weight of points that fall into a group, w_i.

Rather than run generative, can calculate directly.

1.stick l=1
2.generate r.v. beta_1 Beta(1, alpha)
  beta distribution r.v [0,1] by definition.
  break stick at beta_1 == w_1
3.take stick to right, genrate beta2 Beta(1, alpha)
  break stick , w_2 == (1-beta_1)/beta_2

assigning person-data to table-group is equiv. to probaility_[assign_table1] = w_1

"""
# Return a vector of weights drawn from a stick-breaking process
# with dispersion `alpha`.
#
# Recall that the kth weight is
#   \beta_k = (1 - \beta_1) * (1 - \beta_2) * ... * (1 - \beta_{k-1}) * beta_k
# where each $\beta_i$ is drawn from a Beta distribution
#   \beta_i ~ Beta(1, \alpha)
stick_breaking_process = function(num_weights, alpha) {
  betas = rbeta(num_weights, 1, alpha)
  remaining_stick_lengths = c(1, cumprod(1 - betas))[1:num_weights]
  weights = remaining_stick_lengths * betas
  weights
}



#Dirichlet Process Model
"""
http://en.wikipedia.org/wiki/Chinese_restaurant_process#The_Indian_buffet_process
distribution over distribution

Gibbs Sampling:
1. take data set randomly initialize group assignment
2. pick a point, fix the groups of all other points, and in CRP style assign picked point to a new group(existing, or new)
3. will converge

# the group size grows logarithmically
number of clusters converges -- see in histogram

# can look at z-scaled value of each feature in a group

# crp,poly,stick are sequential, dirichlet is parallelizable, (de Finetti theorem)

"""




### z scoring mean 0 std 1 




### indian buffet process


"""
http://ai-ml.blogspot.com/2009/04/more-on-indian-buffet-process.html
http://metaoptimize.com/qa/questions/12397/indian-buffet-the-other-way-around
each person sit at one table
indian each person can sample from multiple tables (belong to multiple clusters)

""" 

### network percolation theory

### cssr
#http://vserver1.cscs.lsa.umich.edu/~crshalizi/CSSR/



### collapse gibbs dirichlet 
"""
http://snippyhollow.github.io/blog/2013/03/10/collapsed-gibbs-sampling-for-dirichlet-process-gaussian-mixture-models/

"""

while (not converged on mus and sigmas):
    for each i = 1 : N in random order do:
        remove x[i]'s sufficient statistics from old cluster z[i]
        if any cluster is empty, remove it and decrease K
        for each k = 1 : K do
            compute P_k(x[i]) = P(x[i] | x[-i]=k)
            N[k,-i] = dim(x[-i]=k)
            compute P(z[i]=k | z[-i], Data) = N[k,-i] / (alpha + N - 1)
        compute P*(x[i]) = P(x[i] | lambda)
        compute P(z[i]=* | z[-i], Data) = alpha / (alpha + N - 1)
        normalize P(z[i] | ...)
        sample z[i] from P(z[i] | ...)
        add x[i]'s sufficient statistics to new cluster z[i]
        (possibly increase K)






### notes

"""
#hashkernel-master/hashkernelalerts.py

#mixed integer programming (probabilistic parsing)


#rubics cube
#alias method

http://www.astroml.org/book_figures/chapter10/fig_autocorrelation.html
time-series
roughly speaking, the term persistence in time series context is often related to the notion of memory properties of time series. to put it another way, you have a persistent time series process if the effect of infinitesimally (very) small shock will be influencing the future predictions of your time series for a very long time. thus the longer the time of influence the longer is the memory and the extremely persistence. you may consider an integrated process i(1) as an example of highly persistent process (information that comes from the shocks never dies out). though fractionally integrated (arfima) processes would be more interesting examples of persistent processes. probably it would be useful to read about measuring conditional persistence in time series in g.kapetanios article.

#global

#decision theory
http://www.gwern.net/Prediction%20markets
solomon puzzle
cancer lesion puzzle
overconfidence effect
"""
